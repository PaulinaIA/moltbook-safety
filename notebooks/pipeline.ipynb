{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Moltbook Karma Data Engineering Pipeline\n",
                "\n",
                "**Objetivo**: Pipeline de ingenieria de datos end-to-end para predecir el karma de usuarios en moltbook.com\n",
                "\n",
                "**Variable Target**: `users.karma` (regresion)\n",
                "\n",
                "[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n",
                "[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n",
                "\n",
                "![Moltbook Banner](../assets/banner.jpg)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Moltbook.com es una red social donde agentes de IA interactúan entre sí de forma autónoma: publican, comentan y votan. Lo interesante es que a veces discuten temas sensibles como vulnerabilidades de sistemas, lo cual genera preocupación sobre cómo estos bots pueden corromperse entre ellos.\n",
                "\n",
                "Este proyecto nace de la necesidad de analizar el **karma** (la reputación de cada agente) para entender qué factores determinan la influencia dentro de la plataforma. Los hallazgos pueden servir para estrategias de seguridad y moderación en sistemas de IA."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Esta red social esta compuesta por:\n",
                "\n",
                "**Entidades**:\n",
                "- `users`: Perfiles de agentes\n",
                "- `posts`: Publicaciones en comunidades\n",
                "- `comments`: Comentarios en posts\n",
                "- `sub_molt`: Comunidades tematicas\n",
                "\n",
                "**Variable Objetivo**: \n",
                "- `karma` : puntuación de reputación del usuario"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configuracion del Entorno"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "En primer lugar configuramos el entorno de trabajo, importando las librerías y configurando el setup necesario para para el desarrollo del proyecto."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.1 Imports y Setup"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Se importan las librerías necesarias, se configura el path y se crean los directorios de trabajo."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Project root: c:\\Users\\Paulina Peralta\\Desktop\\moltbook-karma\n",
                        "Data directory: c:\\Users\\Paulina Peralta\\Desktop\\moltbook-karma\\data\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "from pathlib import Path\n",
                "project_root = Path.cwd().parent\n",
                "if str(project_root) not in sys.path:\n",
                "    sys.path.insert(0, str(project_root))\n",
                "\n",
                "import logging\n",
                "import polars as pl\n",
                "from config.settings import settings\n",
                "\n",
                "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
                "settings.ensure_directories()\n",
                "print(f\"Project root: {settings.project_root}\")\n",
                "print(f\"Data directory: {settings.data_dir}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Web Scraping con Playwright"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Playwright es "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.1 Inicializacion de Base de Datos"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Para ello en primer lugar inicializamos la base de datos mediante la función init_database() y definimos un db_ops para manejar las operaciones con la base de datos."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Database already exists\n",
                        "Current counts - Users: 981, Posts: 1242\n"
                    ]
                }
            ],
            "source": [
                "from src.database.connection import init_database, check_database_exists\n",
                "from src.database.operations import DatabaseOperations\n",
                "from src.database.models import User, Post, Comment, SubMolt\n",
                "\n",
                "if not check_database_exists():\n",
                "    print(\"Initializing database...\")\n",
                "    init_database()\n",
                "else:\n",
                "    print(\"Database already exists\")\n",
                "\n",
                "db_ops = DatabaseOperations()\n",
                "print(f\"Current counts - Users: {db_ops.count(User)}, Posts: {db_ops.count(Post)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Ejecucion del Scraper"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Luego ejecutamos el scraper mediante el método scrape_all, que es el orquestador principal. Su función es ejecutar el flujo completo de recolección de datos: el pipeline scrapea usuarios, submolts y posts, y genera un reporte. ¿Cómo lo hace? Primero busca nuevas URLs de usuarios en la lista global /u, respetando el límite de max_users. Luego procesa cada usuario: entra en cada perfil, descarga el HTML (o utiliza el caché) y extrae datos como karma, nombre, entre otros. Finalmente, guarda y actualiza esta información en la base de datos.\n",
                "\n",
                "Luego, para scrapear submolts, busca nuevas comunidades en la lista global /m, respetando el límite de max_submolts. Se procesa cada submolt entrando a la página de la comunidad y guardando sus datos. Para extraer los posts dentro de la página de cada submolt, lo que hace es buscar y extraer todos los posts viables en esa página, y los guarda en la base de datos asociados a ese submolt y a su autor. Finalmente, genera un reporte consultando la base de datos para contar cuántos posts y comentarios totales hay guardados, y devuelve un diccionario con el resumen: { \"users\": X, \"submolts\": Y, \"posts\": Z, \"comments\": W }."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Exploratory Data Analysis"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Para el análisis exploratorio de los datos utilizamos polars, que es una librería que introduce el concepto de lazyframes, que a diferencia de pandas, que carga y procesa todo en memoria inmediatamente, polars solo lo construye en un plan de ejecución.\n",
                "\n",
                "Permitiendo optimización automática, ya que utilizando .collect() polars analiza todo el plan y lo optimiza antes de la ejecución, y puede combinar filtros eliminando pasos redundantes, leer solo las columnas necesarias, y está diseñado para usar todos los núcleos de la CPU en paralelo.\n",
                "\n",
                "Y al ser lazy, con polars podemos procesar el dataset y cargar los datos en bloques, procesar, y liberar la memoria.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.1 Carga de Datos con Polars"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Para cargar los datos utilizamos polars, donde creamos un **LazyFrame** que lo que hace es solo crear como un plan de acción o un mapa de dónde están los datos, en este caso de users y posts, donde no gasta tanta memoria, lo cual es ideal para trabajar con muchos datos.\n",
                "\n",
                "Luego, con .collect() se da el orden de ejecución, donde ponemos `users_lf.collect()`. Ahí polars se va a la base de datos, lee la información y la convierte en un dataframe listo para usar.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-09 16:44:38,147 - INFO - Loaded 981 rows from users\n",
                        "2026-02-09 16:44:38,167 - INFO - Loaded 1242 rows from posts\n",
                        "2026-02-09 16:44:38,178 - INFO - Loaded 644 rows from comments\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Users: 981 records\n",
                        "Posts: 1242 records\n"
                    ]
                }
            ],
            "source": [
                "from src.processing.silver import load_table_to_lazy\n",
                "users_lf = load_table_to_lazy(\"users\")\n",
                "posts_lf = load_table_to_lazy(\"posts\")\n",
                "comments_lf = load_table_to_lazy(\"comments\")\n",
                "users_df = users_lf.collect()\n",
                "posts_df = posts_lf.collect()\n",
                "print(f\"Users: {len(users_df)} records\")\n",
                "print(f\"Posts: {len(posts_df)} records\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.2 Estadísticas Descriptivas de Karma"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Para poder entender como el Karma esta distribuido imprimimos sus estadísticas y vemos su distribución."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Karma Statistics:\n",
                        "shape: (1, 7)\n",
                        "┌─────────────┬────────┬──────────────┬─────┬────────┬─────┬─────┐\n",
                        "│ mean        ┆ median ┆ std          ┆ min ┆ max    ┆ q25 ┆ q75 │\n",
                        "│ ---         ┆ ---    ┆ ---          ┆ --- ┆ ---    ┆ --- ┆ --- │\n",
                        "│ f64         ┆ f64    ┆ f64          ┆ i64 ┆ i64    ┆ f64 ┆ f64 │\n",
                        "╞═════════════╪════════╪══════════════╪═════╪════════╪═════╪═════╡\n",
                        "│ 6553.063201 ┆ 0.0    ┆ 44460.967544 ┆ 0   ┆ 500002 ┆ 0.0 ┆ 0.0 │\n",
                        "└─────────────┴────────┴──────────────┴─────┴────────┴─────┴─────┘\n"
                    ]
                }
            ],
            "source": [
                "karma_stats = users_df.select([\n",
                "    pl.col(\"karma\").mean().alias(\"mean\"),\n",
                "    pl.col(\"karma\").median().alias(\"median\"),\n",
                "    pl.col(\"karma\").std().alias(\"std\"),\n",
                "    pl.col(\"karma\").min().alias(\"min\"),\n",
                "    pl.col(\"karma\").max().alias(\"max\"),\n",
                "    pl.col(\"karma\").quantile(0.25).alias(\"q25\"),\n",
                "    pl.col(\"karma\").quantile(0.75).alias(\"q75\"),\n",
                "])\n",
                "print(\"Karma Statistics:\")\n",
                "print(karma_stats)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Se visualiza que la distribución está muy sesgada con mediana igual a cero y un máximo mayor a 500K. Donde mayoría de usuarios tienen karma bajo y unos pocos acumulan valores extremos."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.3 Distribución de Variables"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Luego visualizamos la distribución de las variables categóricas."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "User Profile Statistics:\n",
                        "shape: (1, 4)\n",
                        "┌───────────────┬───────────────┬────────────────────────┬──────────────────┐\n",
                        "│ avg_followers ┆ avg_following ┆ users_with_description ┆ users_with_owner │\n",
                        "│ ---           ┆ ---           ┆ ---                    ┆ ---              │\n",
                        "│ f64           ┆ f64           ┆ u32                    ┆ u32              │\n",
                        "╞═══════════════╪═══════════════╪════════════════════════╪══════════════════╡\n",
                        "│ 0.462793      ┆ 0.046891      ┆ 41                     ┆ 41               │\n",
                        "└───────────────┴───────────────┴────────────────────────┴──────────────────┘\n"
                    ]
                }
            ],
            "source": [
                "user_stats = users_df.select([\n",
                "    pl.col(\"followers\").mean().alias(\"avg_followers\"),\n",
                "    pl.col(\"following\").mean().alias(\"avg_following\"),\n",
                "    pl.col(\"description\").is_not_null().sum().alias(\"users_with_description\"),\n",
                "    pl.col(\"human_owner\").is_not_null().sum().alias(\"users_with_owner\"),\n",
                "])\n",
                "\n",
                "print(\"User Profile Statistics:\")\n",
                "print(user_stats)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Limpieza y Preparacion de Datos"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.1 Pipeline de Limpieza con Polars Lazy"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Creamos una función build_silver_layer() que lo que hace es tomar los datos crudos que vienen de nuestra base de datos, donde se limpian y se estandarizan los datos, convirtiendo fechas, arreglando textos y asegurando los tipos de datos correctos. Luego almacenamos en parquet, que exporta estos datos limpios a archivos .parquet en la carpeta data/silver."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-09 16:44:47,539 - INFO - Loaded 981 rows from users\n",
                        "2026-02-09 16:44:47,619 - INFO - Wrote 981 users to c:\\Users\\Paulina Peralta\\Desktop\\moltbook-karma\\data\\silver\\users.parquet\n",
                        "2026-02-09 16:44:47,639 - INFO - Loaded 1242 rows from posts\n",
                        "2026-02-09 16:44:47,671 - INFO - Wrote 1242 posts to c:\\Users\\Paulina Peralta\\Desktop\\moltbook-karma\\data\\silver\\posts.parquet\n",
                        "2026-02-09 16:44:47,682 - INFO - Loaded 644 rows from comments\n",
                        "2026-02-09 16:44:47,692 - INFO - Wrote 644 comments to c:\\Users\\Paulina Peralta\\Desktop\\moltbook-karma\\data\\silver\\comments.parquet\n",
                        "2026-02-09 16:44:47,700 - INFO - Loaded 55 rows from sub_molt\n",
                        "2026-02-09 16:44:47,706 - INFO - Wrote 55 submolts to c:\\Users\\Paulina Peralta\\Desktop\\moltbook-karma\\data\\silver\\submolts.parquet\n",
                        "2026-02-09 16:44:47,709 - INFO - Silver layer build complete: {'users': 981, 'posts': 1242, 'comments': 644, 'submolts': 55}\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Silver Layer Built:\n",
                        "  users: 981 records\n",
                        "  posts: 1242 records\n",
                        "  comments: 644 records\n",
                        "  submolts: 55 records\n",
                        "\n",
                        "Silver users sample:\n",
                        "shape: (3, 9)\n",
                        "┌─────────────┬─────────────┬───────┬────────────┬───┬────────┬───────────┬───────────┬────────────┐\n",
                        "│ id_user     ┆ name        ┆ karma ┆ descriptio ┆ … ┆ joined ┆ followers ┆ following ┆ scraped_at │\n",
                        "│ ---         ┆ ---         ┆ ---   ┆ n          ┆   ┆ ---    ┆ ---       ┆ ---       ┆ ---        │\n",
                        "│ str         ┆ str         ┆ i64   ┆ ---        ┆   ┆ str    ┆ i64       ┆ i64       ┆ str        │\n",
                        "│             ┆             ┆       ┆ str        ┆   ┆        ┆           ┆           ┆            │\n",
                        "╞═════════════╪═════════════╪═══════╪════════════╪═══╪════════╪═══════════╪═══════════╪════════════╡\n",
                        "│ user_7ca81d ┆ AureliusPro ┆ 0     ┆            ┆ … ┆ null   ┆ 0         ┆ 0         ┆ 2026-02-09 │\n",
                        "│ d06237      ┆ tocol       ┆       ┆            ┆   ┆        ┆           ┆           ┆ T19:05:22. │\n",
                        "│             ┆             ┆       ┆            ┆   ┆        ┆           ┆           ┆ 608166     │\n",
                        "│ user_57908f ┆ ZenithGarci ┆ 0     ┆            ┆ … ┆ null   ┆ 0         ┆ 0         ┆ 2026-02-07 │\n",
                        "│ 6a36b5      ┆ a           ┆       ┆            ┆   ┆        ┆           ┆           ┆ T23:37:33. │\n",
                        "│             ┆             ┆       ┆            ┆   ┆        ┆           ┆           ┆ 673358     │\n",
                        "│ user_ed278b ┆ NimbusDrift ┆ 0     ┆            ┆ … ┆ null   ┆ 0         ┆ 0         ┆ 2026-02-09 │\n",
                        "│ 7a8e0e      ┆ s           ┆       ┆            ┆   ┆        ┆           ┆           ┆ T19:03:14. │\n",
                        "│             ┆             ┆       ┆            ┆   ┆        ┆           ┆           ┆ 519175     │\n",
                        "└─────────────┴─────────────┴───────┴────────────┴───┴────────┴───────────┴───────────┴────────────┘\n"
                    ]
                }
            ],
            "source": [
                "from src.processing.silver import build_silver_layer\n",
                "\n",
                "silver_results = build_silver_layer()\n",
                "print(\"Silver Layer Built:\")\n",
                "for table, count in silver_results.items():\n",
                "    print(f\"  {table}: {count} records\")\n",
                "\n",
                "silver_users = pl.read_parquet(settings.silver_dir / \"users.parquet\")\n",
                "print(f\"\\nSilver users sample:\")\n",
                "print(silver_users.head(3))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Aqui se visualizan los resultados de la limpieza, y se confirma que se limpio correctamente, donde se genero una base de datos analitica y limpia para ser utilizada."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Feature Engineering (Gold Layer)\n",
                "\n",
                "### 5.1 Ingenieria de Features con Polars Lazy"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Se realizo el uso de Polars debido a que nos encontramos con un dataset enorme . Para esto se realiza la carga del gold layer. En esta layer transformaremos datos limpios en features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-09 16:44:52,150 - INFO - Loaded users from silver layer\n",
                        "2026-02-09 16:44:52,152 - INFO - Loaded posts from silver layer\n",
                        "2026-02-09 16:44:52,152 - INFO - Loaded comments from silver layer\n",
                        "2026-02-09 16:44:52,154 - INFO - Loaded submolts from silver layer\n",
                        "2026-02-09 16:44:52,237 - INFO - Wrote 981 user feature records with 21 columns to c:\\Users\\Paulina Peralta\\Desktop\\moltbook-karma\\data\\gold\\user_features.parquet\n",
                        "2026-02-09 16:44:52,239 - INFO - Feature columns: ['id_user', 'name', 'karma', 'followers', 'following', 'follower_ratio', 'has_description', 'has_human_owner', 'description_length', 'post_count', 'total_post_rating', 'avg_post_rating', 'max_post_rating', 'avg_title_length', 'avg_post_desc_length', 'comment_count', 'total_comment_rating', 'avg_comment_rating', 'avg_comment_length', 'total_activity', 'total_rating']\n",
                        "2026-02-09 16:44:52,239 - INFO - Gold layer build complete: {'user_features': 981, 'feature_columns': 21}\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Gold Layer Built:\n",
                        "  User features: 981 records\n",
                        "  Feature columns: 21\n"
                    ]
                }
            ],
            "source": [
                "from src.processing.gold import build_gold_layer\n",
                "\n",
                "gold_results = build_gold_layer()\n",
                "print(\"Gold Layer Built:\")\n",
                "print(f\"  User features: {gold_results['user_features']} records\")\n",
                "print(f\"  Feature columns: {gold_results['feature_columns']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 Descripcion de Features"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Las features creadas son:\n",
                "\n",
                "- **Perfil**: followers, following, follower_ratio (métricas de red social)\n",
                "- **Contenido**: has_description, description_length (qué tan completo está el perfil)\n",
                "- **Actividad en Posts**: post_count, avg_post_rating, total_post_rating\n",
                "- **Actividad en Comentarios**: comment_count, avg_comment_rating\n",
                "- **Agregadas**: total_activity, total_rating (métricas combinadas)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Feature Columns:\n",
                        "  id_user: String\n",
                        "  name: String\n",
                        "  karma: Int64\n",
                        "  followers: Int64\n",
                        "  following: Int64\n",
                        "  follower_ratio: Float64\n",
                        "  has_description: Int32\n",
                        "  has_human_owner: Int32\n",
                        "  description_length: UInt32\n",
                        "  post_count: UInt32\n",
                        "  total_post_rating: Int64\n",
                        "  avg_post_rating: Float64\n",
                        "  max_post_rating: Int64\n",
                        "  avg_title_length: Float64\n",
                        "  avg_post_desc_length: Float64\n",
                        "  comment_count: UInt32\n",
                        "  total_comment_rating: Int64\n",
                        "  avg_comment_rating: Float64\n",
                        "  avg_comment_length: Float64\n",
                        "  total_activity: UInt32\n",
                        "  total_rating: Int64\n",
                        "\n",
                        "Feature Statistics:\n",
                        "shape: (9, 22)\n",
                        "┌────────────┬────────────┬──────┬────────────┬───┬────────────┬───────────┬───────────┬───────────┐\n",
                        "│ statistic  ┆ id_user    ┆ name ┆ karma      ┆ … ┆ avg_commen ┆ avg_comme ┆ total_act ┆ total_rat │\n",
                        "│ ---        ┆ ---        ┆ ---  ┆ ---        ┆   ┆ t_rating   ┆ nt_length ┆ ivity     ┆ ing       │\n",
                        "│ str        ┆ str        ┆ str  ┆ f64        ┆   ┆ ---        ┆ ---       ┆ ---       ┆ ---       │\n",
                        "│            ┆            ┆      ┆            ┆   ┆ f64        ┆ f64       ┆ f64       ┆ f64       │\n",
                        "╞════════════╪════════════╪══════╪════════════╪═══╪════════════╪═══════════╪═══════════╪═══════════╡\n",
                        "│ count      ┆ 981        ┆ 981  ┆ 981.0      ┆ … ┆ 981.0      ┆ 981.0     ┆ 981.0     ┆ 981.0     │\n",
                        "│ null_count ┆ 0          ┆ 0    ┆ 0.0        ┆ … ┆ 0.0        ┆ 0.0       ┆ 0.0       ┆ 0.0       │\n",
                        "│ mean       ┆ null       ┆ null ┆ 6553.06320 ┆ … ┆ 0.0        ┆ 57.64881  ┆ 1.922528  ┆ 40.981651 │\n",
                        "│            ┆            ┆      ┆ 1          ┆   ┆            ┆           ┆           ┆           │\n",
                        "│ std        ┆ null       ┆ null ┆ 44460.9675 ┆ … ┆ 0.0        ┆ 129.37179 ┆ 3.009867  ┆ 189.76306 │\n",
                        "│            ┆            ┆      ┆ 44         ┆   ┆            ┆ 5         ┆           ┆ 8         │\n",
                        "│ min        ┆ user_00234 ┆ 0x96 ┆ 0.0        ┆ … ┆ 0.0        ┆ 0.0       ┆ 0.0       ┆ 0.0       │\n",
                        "│            ┆ 0b41aee    ┆      ┆            ┆   ┆            ┆           ┆           ┆           │\n",
                        "│ 25%        ┆ null       ┆ null ┆ 0.0        ┆ … ┆ 0.0        ┆ 0.0       ┆ 1.0       ┆ 0.0       │\n",
                        "│ 50%        ┆ null       ┆ null ┆ 0.0        ┆ … ┆ 0.0        ┆ 0.0       ┆ 1.0       ┆ 9.0       │\n",
                        "│ 75%        ┆ null       ┆ null ┆ 0.0        ┆ … ┆ 0.0        ┆ 54.0      ┆ 2.0       ┆ 19.0      │\n",
                        "│ max        ┆ user_ffbc3 ┆ zm11 ┆ 500002.0   ┆ … ┆ 0.0        ┆ 1314.0    ┆ 34.0      ┆ 3590.0    │\n",
                        "│            ┆ c2957cb    ┆      ┆            ┆   ┆            ┆           ┆           ┆           │\n",
                        "└────────────┴────────────┴──────┴────────────┴───┴────────────┴───────────┴───────────┴───────────┘\n"
                    ]
                }
            ],
            "source": [
                "from src.processing.gold import get_modeling_data\n",
                "\n",
                "features_df = get_modeling_data()\n",
                "print(\"Feature Columns:\")\n",
                "for col in features_df.columns:\n",
                "    dtype = features_df[col].dtype\n",
                "    print(f\"  {col}: {dtype}\")\n",
                "print(f\"\\nFeature Statistics:\")\n",
                "print(features_df.describe())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Se observa que hay mucha variabilidad en los datos :\n",
                "Se obtuvieron 981 registros, 20 columnas"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Modelado con H2O AutoML\n",
                "\n",
                "### 6.1 Entrenamiento del Modelo"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Se utiliza **H2O AutoML** porque automatiza la selección y optimización de modelos. Entrena varios algoritmos (GBM, Random Forest, Deep Learning, GLM) y elige el mejor según la métrica objetivo."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training H2O AutoML model...\n",
                        "Features: ['followers', 'following', 'follower_ratio', 'has_description', 'has_human_owner', 'description_length', 'post_count', 'total_post_rating', 'avg_post_rating', 'max_post_rating', 'avg_title_length', 'avg_post_desc_length', 'comment_count', 'total_comment_rating', 'avg_comment_rating', 'avg_comment_length', 'total_activity', 'total_rating']\n",
                        "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
                        "Attempting to start a local H2O server...\n",
                        "; Java HotSpot(TM) 64-Bit Server VM (build 25.461-b11, mixed mode)\n",
                        "  Starting server from C:\\Users\\Paulina Peralta\\anaconda3\\Lib\\site-packages\\h2o\\backend\\bin\\h2o.jar\n",
                        "  Ice root: C:\\Users\\PAULIN~1\\AppData\\Local\\Temp\\tmppivf28ph\n",
                        "  JVM stdout: C:\\Users\\PAULIN~1\\AppData\\Local\\Temp\\tmppivf28ph\\h2o_Paulina_Peralta_started_from_python.out\n",
                        "  JVM stderr: C:\\Users\\PAULIN~1\\AppData\\Local\\Temp\\tmppivf28ph\\h2o_Paulina_Peralta_started_from_python.err\n",
                        "  Server is running at http://127.0.0.1:54321\n",
                        "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "<style>\n",
                            "\n",
                            "#h2o-table-1.h2o-container {\n",
                            "  overflow-x: auto;\n",
                            "}\n",
                            "#h2o-table-1 .h2o-table {\n",
                            "  /* width: 100%; */\n",
                            "  margin-top: 1em;\n",
                            "  margin-bottom: 1em;\n",
                            "}\n",
                            "#h2o-table-1 .h2o-table caption {\n",
                            "  white-space: nowrap;\n",
                            "  caption-side: top;\n",
                            "  text-align: left;\n",
                            "  /* margin-left: 1em; */\n",
                            "  margin: 0;\n",
                            "  font-size: larger;\n",
                            "}\n",
                            "#h2o-table-1 .h2o-table thead {\n",
                            "  white-space: nowrap; \n",
                            "  position: sticky;\n",
                            "  top: 0;\n",
                            "  box-shadow: 0 -1px inset;\n",
                            "}\n",
                            "#h2o-table-1 .h2o-table tbody {\n",
                            "  overflow: auto;\n",
                            "}\n",
                            "#h2o-table-1 .h2o-table th,\n",
                            "#h2o-table-1 .h2o-table td {\n",
                            "  text-align: right;\n",
                            "  /* border: 1px solid; */\n",
                            "}\n",
                            "#h2o-table-1 .h2o-table tr:nth-child(even) {\n",
                            "  /* background: #F5F5F5 */\n",
                            "}\n",
                            "\n",
                            "</style>      \n",
                            "<div id=\"h2o-table-1\" class=\"h2o-container\">\n",
                            "  <table class=\"h2o-table\">\n",
                            "    <caption></caption>\n",
                            "    <thead></thead>\n",
                            "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
                            "<td>07 secs</td></tr>\n",
                            "<tr><td>H2O_cluster_timezone:</td>\n",
                            "<td>-03:00</td></tr>\n",
                            "<tr><td>H2O_data_parsing_timezone:</td>\n",
                            "<td>UTC</td></tr>\n",
                            "<tr><td>H2O_cluster_version:</td>\n",
                            "<td>3.46.0.9</td></tr>\n",
                            "<tr><td>H2O_cluster_version_age:</td>\n",
                            "<td>2 months and 16 days</td></tr>\n",
                            "<tr><td>H2O_cluster_name:</td>\n",
                            "<td>H2O_from_python_Paulina_Peralta_lt8mxu</td></tr>\n",
                            "<tr><td>H2O_cluster_total_nodes:</td>\n",
                            "<td>1</td></tr>\n",
                            "<tr><td>H2O_cluster_free_memory:</td>\n",
                            "<td>3.549 Gb</td></tr>\n",
                            "<tr><td>H2O_cluster_total_cores:</td>\n",
                            "<td>16</td></tr>\n",
                            "<tr><td>H2O_cluster_allowed_cores:</td>\n",
                            "<td>16</td></tr>\n",
                            "<tr><td>H2O_cluster_status:</td>\n",
                            "<td>locked, healthy</td></tr>\n",
                            "<tr><td>H2O_connection_url:</td>\n",
                            "<td>http://127.0.0.1:54321</td></tr>\n",
                            "<tr><td>H2O_connection_proxy:</td>\n",
                            "<td>{\"http\": null, \"https\": null}</td></tr>\n",
                            "<tr><td>H2O_internal_security:</td>\n",
                            "<td>False</td></tr>\n",
                            "<tr><td>Python_version:</td>\n",
                            "<td>3.9.12 final</td></tr></tbody>\n",
                            "  </table>\n",
                            "</div>\n"
                        ],
                        "text/plain": [
                            "--------------------------  --------------------------------------\n",
                            "H2O_cluster_uptime:         07 secs\n",
                            "H2O_cluster_timezone:       -03:00\n",
                            "H2O_data_parsing_timezone:  UTC\n",
                            "H2O_cluster_version:        3.46.0.9\n",
                            "H2O_cluster_version_age:    2 months and 16 days\n",
                            "H2O_cluster_name:           H2O_from_python_Paulina_Peralta_lt8mxu\n",
                            "H2O_cluster_total_nodes:    1\n",
                            "H2O_cluster_free_memory:    3.549 Gb\n",
                            "H2O_cluster_total_cores:    16\n",
                            "H2O_cluster_allowed_cores:  16\n",
                            "H2O_cluster_status:         locked, healthy\n",
                            "H2O_connection_url:         http://127.0.0.1:54321\n",
                            "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
                            "H2O_internal_security:      False\n",
                            "Python_version:             3.9.12 final\n",
                            "--------------------------  --------------------------------------"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-09 16:46:20,011 - INFO - H2O initialized\n",
                        "2026-02-09 16:46:20,011 - INFO - Training with 18 features, 981 samples\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-09 16:46:21,670 - INFO - Train size: 773, Test size: 208\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "AutoML progress: |\n",
                        "16:46:21.780: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n",
                        "16:46:21.799: AutoML: XGBoost is not available; skipping it.\n",
                        "16:46:21.869: _train param, Dropping bad and constant columns: [avg_comment_rating, total_comment_rating]\n",
                        "\n",
                        "\n",
                        "16:46:22.530: _train param, Dropping bad and constant columns: [avg_comment_rating, total_comment_rating]\n",
                        "\n",
                        "█\n",
                        "16:46:23.693: _train param, Dropping bad and constant columns: [avg_comment_rating, total_comment_rating]\n",
                        "\n",
                        "█\n",
                        "16:46:24.122: _train param, Dropping bad and constant columns: [avg_comment_rating, total_comment_rating]\n",
                        "\n",
                        "█\n",
                        "16:46:24.630: _train param, Dropping bad and constant columns: [avg_comment_rating, total_comment_rating]\n",
                        "16:46:24.933: _train param, Dropping bad and constant columns: [avg_comment_rating, total_comment_rating]\n",
                        "\n",
                        "██\n",
                        "16:46:25.149: _train param, Dropping bad and constant columns: [avg_comment_rating, total_comment_rating]\n",
                        "\n",
                        "███\n",
                        "16:46:25.537: _train param, Dropping bad and constant columns: [avg_comment_rating, total_comment_rating]\n",
                        "\n",
                        "█████\n",
                        "16:46:26.461: _train param, Dropping unused columns: [avg_comment_rating, total_comment_rating]\n",
                        "16:46:26.979: _train param, Dropping unused columns: [avg_comment_rating, total_comment_rating]\n",
                        "\n",
                        "██████████████████████████████████████████████████| (done) 100%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-09 16:46:28,869 - INFO - Best model: GBM_2_AutoML_1_20260209_164621\n",
                        "2026-02-09 16:46:28,889 - INFO - Training complete - MAE: 3234.8072, RMSE: 23050.9321, R2: 0.6363\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Best Model: GBM_2_AutoML_1_20260209_164621\n"
                    ]
                }
            ],
            "source": [
                "from src.models.trainer import H2OTrainer, FEATURE_COLUMNS\n",
                "\n",
                "trainer = H2OTrainer(max_models=10,max_runtime_secs=300,)\n",
                "print(\"Training H2O AutoML model...\")\n",
                "print(f\"Features: {FEATURE_COLUMNS}\")\n",
                "results = trainer.train(data=features_df,target=\"karma\",features=FEATURE_COLUMNS,)\n",
                "print(f\"\\nBest Model: {results['model_id']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Se concluye que el best model es el GBM_2_AutoML. Esto quiere decir que es la version 2 del modelo GBM, el algoritmo usado es el gradient boosting y la mejora es que ayuda a corregir el error (real - preddicion ) del primer modelo\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.2 Evaluacion del Modelo"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Se calculan los indicadores para poder evaluar el modelo."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model Evaluation Metrics:\n",
                        "  MAE:  3234.8072\n",
                        "  RMSE: 23050.9321\n",
                        "  R2:   0.6363\n",
                        "\n",
                        "  Train samples: 773\n",
                        "  Test samples:  208\n"
                    ]
                }
            ],
            "source": [
                "print(\"Model Evaluation Metrics:\")\n",
                "print(f\"  MAE:  {results['mae']:.4f}\")\n",
                "print(f\"  RMSE: {results['rmse']:.4f}\")\n",
                "print(f\"  R2:   {results['r2']:.4f}\")\n",
                "print(f\"\\n  Train samples: {results['train_size']}\")\n",
                "print(f\"  Test samples:  {results['test_size']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Donde:\n",
                "\n",
                "- **R² = 0.64**: el modelo explica el 64% de la varianza del karma. \n",
                "- **MAE = 3,235**: en promedio se equivoca por ~3K puntos de karma.\n",
                "- **RMSE alto**: significa que hay outliers extremos en la variable objetivo. Los usuarios que tienen un Karma muy alto."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.3 Predicciones"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Se realizan las predicciones del modelo."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
                        "gbm prediction progress: |███████████████████████████████████████████████████████| (done) 100%\n",
                        "Actual vs Predicted Karma:\n",
                        "shape: (10, 3)\n",
                        "┌──────────────────┬───────┬─────────────────┐\n",
                        "│ name             ┆ karma ┆ karma_predicted │\n",
                        "│ ---              ┆ ---   ┆ ---             │\n",
                        "│ str              ┆ i64   ┆ f64             │\n",
                        "╞══════════════════╪═══════╪═════════════════╡\n",
                        "│ AureliusProtocol ┆ 0     ┆ 176.243253      │\n",
                        "│ ZenithGarcia     ┆ 0     ┆ 176.243253      │\n",
                        "│ NimbusDrifts     ┆ 0     ┆ 176.243253      │\n",
                        "│ MBC20MintPoster  ┆ 0     ┆ 176.243253      │\n",
                        "│ DivineLuna       ┆ 0     ┆ 176.243253      │\n",
                        "│ Clawd            ┆ 0     ┆ 176.243253      │\n",
                        "│ NebulaBot2026    ┆ 0     ┆ 176.243253      │\n",
                        "│ Virgil_DT        ┆ 0     ┆ 176.243253      │\n",
                        "│ Shellraiser      ┆ 0     ┆ 176.243253      │\n",
                        "│ PenkoAI          ┆ 0     ┆ 176.243253      │\n",
                        "└──────────────────┴───────┴─────────────────┘\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\Paulina Peralta\\anaconda3\\lib\\site-packages\\h2o\\frame.py:1983: H2ODependencyWarning: Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using multi-thread, install polars and pyarrow and use it as pandas_df = h2o_df.as_data_frame(use_multi_thread=True)\n",
                        "\n",
                        "  warnings.warn(\"Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using\"\n"
                    ]
                }
            ],
            "source": [
                "predictions = trainer.predict(features_df)\n",
                "comparison = predictions.select([\"name\", \"karma\", \"karma_predicted\"]).head(10)\n",
                "print(\"Actual vs Predicted Karma:\")\n",
                "print(comparison)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Se almacena el modelo entrenado."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-09 16:46:54,142 - INFO - Saved model to C:\\Users\\Paulina Peralta\\Desktop\\moltbook-karma\\data\\models\\GBM_2_AutoML_1_20260209_164621\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model saved to: C:\\Users\\Paulina Peralta\\Desktop\\moltbook-karma\\data\\models\\GBM_2_AutoML_1_20260209_164621\n",
                        "Predictions saved to: c:\\Users\\Paulina Peralta\\Desktop\\moltbook-karma\\data\\models\\predictions.parquet\n"
                    ]
                }
            ],
            "source": [
                "model_path = trainer.save_model()\n",
                "print(f\"Model saved to: {model_path}\")\n",
                "pred_path = settings.models_dir / \"predictions.parquet\"\n",
                "predictions.write_parquet(pred_path)\n",
                "print(f\"Predictions saved to: {pred_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Conclusiones\n",
                "\n",
                "### 7.1 Resumen del Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "==================================================\n",
                        "RESUMEN DEL PIPELINE\n",
                        "==================================================\n",
                        "\n",
                        "1. Web Scraping:\n",
                        "   - Usuarios: 981\n",
                        "   - Posts: 1242\n",
                        "   - SubMolts: 55\n",
                        "\n",
                        "2. Procesamiento:\n",
                        "   - Silver layer: c:\\Users\\Paulina Peralta\\Desktop\\moltbook-karma\\data\\silver\n",
                        "   - Gold layer: c:\\Users\\Paulina Peralta\\Desktop\\moltbook-karma\\data\\gold\n",
                        "\n",
                        "3. Modelado:\n",
                        "   - Algoritmo: H2O AutoML\n",
                        "   - Target: karma (regresion)\n",
                        "   - MAE: 3234.8072\n",
                        "   - R2: 0.6363\n",
                        "\n",
                        "==================================================\n"
                    ]
                }
            ],
            "source": [
                "print(\"=\" * 50)\n",
                "print(\"RESUMEN DEL PIPELINE\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"\\n1. Web Scraping:\")\n",
                "print(f\"   - Usuarios: {db_ops.count(User)}\")\n",
                "print(f\"   - Posts: {db_ops.count(Post)}\")\n",
                "print(f\"   - SubMolts: {db_ops.count(SubMolt)}\")\n",
                "print(f\"\\n2. Procesamiento:\")\n",
                "print(f\"   - Silver layer: {settings.silver_dir}\")\n",
                "print(f\"   - Gold layer: {settings.gold_dir}\")\n",
                "print(f\"\\n3. Modelado:\")\n",
                "print(f\"   - Algoritmo: H2O AutoML\")\n",
                "print(f\"   - Target: karma (regresion)\")\n",
                "print(f\"   - MAE: {results.get('mae', 'N/A'):.4f}\")\n",
                "print(f\"   - R2: {results.get('r2', 'N/A'):.4f}\")\n",
                "print(\"\\n\" + \"=\" * 50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "* H20 AutoML pudo elegir un algoritmo optimo\n",
                "* Las features engineered relevantes fueron follower_ratio, total_activity,\n",
                "* El modelo llega a ser un buen predictor debido a la elecciondel AutoMl puesto que mas 63% de la varianza se encuentra explicada por el modelo\n",
                "* El MAE salio alto (3234.8) Esto quiere decir que el modelo predice ±3234 puntos de karma de diferencia. Esto tambien puede ser ya que se extrajo moltobook user con bastante karma como nuevos, por lo que se podria mejorar cuando se extraiga mas usuarios\n",
                "\n",
                "Por todo ello podemos decir que la relacion usuario con el karma puede llegar a ser predecible"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7.2 Observaciones\n",
                "\n",
                "1. **Web Scraping**: Se realizo el web scrapping en un proyecto modular en dodne se dividio por clases cada parte como usuarios, en donde se uso las herramientas como beautiful soup, se considero :\n",
                "•⁠  ⁠Hacer un webscrapping para usuarios nuevos y usuarios con alto karma, es por ello que se observa una gran variabilidad en ellos\n",
                "•⁠  ⁠Hacer un webscrapping para submolt, en donde en ecada submolt se encuentran los post mas rankeados y dentro de ellos los comentarios mas rankeados. Por temas del scope del trabajo se determino un limite por cada estructura de datos anidada.\n",
                "\n",
                "2. **Procesamiento con Polars**: Lazy evaluation optimiza memoria y rendimiento\n",
                "\n",
                "3. **Feature Engineering**: Las features derivadas (follower_ratio, total_activity) capturan engagement\n",
                "\n",
                "4. **Modelado**: H2O AutoML automatiza la seleccion del mejor algoritmo\n",
                "\n",
                "5. **Limitaciones**: \n",
                "   - El dataset es pequeno para produccion\n",
                "   - Algunas features pueden tener alta correlacion\n",
                "   - El karma puede depender de factores no capturados (tiempo en plataforma, calidad de contenido)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
