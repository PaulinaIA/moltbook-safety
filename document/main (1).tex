\documentclass[12pt, a4paper]{article}

% ==================== CODIFICACIÓN Y LENGUAJE ====================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[spanish]{babel}

% ==================== GEOMETRÍA Y FORMATO ====================
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{parskip}
\usepackage{setspace}
\onehalfspacing

% ==================== TIPOGRAFÍA ====================
\usepackage{lmodern}

% ==================== MATEMÁTICAS ====================
\usepackage{amsmath, amssymb}

% ==================== GRÁFICOS Y FIGURAS ====================
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}

% ==================== TABLAS ====================
\usepackage{array, booktabs}
\usepackage{tabularx}

% ==================== COLORES ====================

\usepackage[dvipsnames, table]{xcolor}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{1.0, 1.0, 1.0}
\definecolor{dockerblue}{HTML}{2496ED}

% ==================== CODIGO FUENTE ====================
\usepackage{listings}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    %keywordstyle=\color{magenta},
    keywordstyle=\color{blue!80!black}\bfseries,
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    %basicstyle=\ttfamily\small,
    basicstyle=\ttfamily\scriptsize	,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single,
    xleftmargin=2em,
    framexleftmargin=1.5em
}
\lstset{style=mystyle}

\lstdefinelanguage{Dockerfile}{
    keywords={FROM, RUN, CMD, LABEL, EXPOSE, ENV, ADD, COPY, ENTRYPOINT, VOLUME, USER, WORKDIR, ARG, HEALTHCHECK},
    keywordstyle=\color{dockerblue}\bfseries,
    sensitive=false,
    comment=[l]{\#},
    commentstyle=\color{codegreen}\ttfamily,
    stringstyle=\color{codepurple}\ttfamily,
    morestring=[b]',
    morestring=[b]"
}

\lstdefinelanguage{yaml}{
    keywords={true,false,null},
    keywordstyle=\color{darkgray}\bfseries,
    sensitive=false,
    comment=[l]{\#},
    commentstyle=\color{codegreen}\ttfamily,
    stringstyle=\color{codepurple}\ttfamily,
    morestring=[b]',
    morestring=[b]"
}
% ==================== ENLACES ====================
\usepackage[hidelinks, colorlinks=true, linkcolor=blue!70!black, urlcolor=blue!70!black, citecolor=blue!70!black]{hyperref}

% ==================== ENCABEZADOS ====================
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small\leftmark}
\fancyhead[R]{\small Dark Eye Core}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% ==================== DOCUMENTO ====================
\begin{document}

% ---------- PORTADA ----------
\begin{titlepage}
    \centering
    \vspace*{0.5cm}
    
    \includegraphics[width=5.5cm]{LaSalleBCN.png}
    
    \vspace{1cm}
    
    {\scshape\large Máster Universitario en Ciencia de Datos\\[0.2cm]
    Universitat Ramon Llull - La Salle\par}
    
    \vspace{1.5cm}
    
    \rule{\textwidth}{1.5pt}\\[0.4cm]
    {\LARGE\bfseries Despliegue de Pipeline ETL con Docker\par}
    \vspace{0.3cm}
    
    % ===== LOGO DE DARK EYE AQUÍ =====
    \includegraphics[width=2.5cm]{ChatGPT Image Nov 29, 2025, 06_05_34 PM.png}
    %\vspace{0.3cm}
    
    {\large\itshape Dark Eye Core\par}
    \rule{\textwidth}{1.5pt}
    
    \vspace{1.5cm}
    
    {\large Infraestructura de Datos\par}
    
    \vfill
    
    {\large
    Paulina Peralta \\[0.4cm]
    30 de noviembre de 2025
    }
    
\end{titlepage}

% ---------- ÍNDICE ----------
\tableofcontents
\clearpage

% ============================================================
\section{Introducción}

\subsection{Contexto del Proyecto}

Dark Eye Core es un proyecto que busca captar el núcleo de los problemas que hay en internet. Por esta razón el proyecto se centra en las amenazas que hay en la red. Actualmente el panorama de la ciberseguridad enfrenta un volumen muy grande de amenazas, lo que hace que sea esencial almacenar amenazas de diversas fuentes para entender cómo se comporta la criminalidad en internet y poder capturar patrones. Lo que hace que sea fundamental contar con sistemas automatizados capaces de extraer esta información, transformarlas, almacenarlas y visualizarlas para poder analizar el núcleo de estas amenazas y así hacer que el internet pueda ser un lugar más seguro para compartir información.

Este proyecto lleva a cabo el desafío de extraer datos de fuentes que ofrecen sus APIs gratuitas como AbuseIPDB para direcciones IP maliciosas, URLhaus para URLs peligrosas asociadas con distribución de malware, y AlienVault OTX (Open Threat Exchange) para diversos indicadores de amenazas.

El proyecto está en un contenedor Docker, con los paquetes necesarios para que se despliegue en cualquier máquina y en diferentes entornos.

\subsection{Objetivos}

Los objetivos principales de este proyecto son:

\begin{itemize}
    \item Desarrollar un pipeline ETL modular que soporte distintas fuentes de datos, que sea reproducible y expandible.
    \item Empaquetar el pipeline ETL en un contenedor Docker.
    \item Orquestar servicios interconectados con Docker Compose: pipeline ETL, PostgreSQL y Grafana.
    \item Automatizar la ejecucion del pipeline con Apache Airflow mediante DAGs diarios.
    \item Implementar un panel de visualización con Grafana, para monitorear indicadores de amenazas.
\end{itemize}

% \subsection{Alcance}

% Este proyecto explica todo el proceso de
% Este informe cubre el proceso completo de contenerización del pipeline Dark Eye: selección y justificación de la imagen base, explicación detallada de cada instrucción del Dockerfile, descripción de dependencias Python, configuración de Docker Compose y documentación de la estructura del código.

% ============================================================
\clearpage

\section{Arquitectura del Sistema}

\subsection{Visión General ETL}

El pipeline tiene cuatro módulos principales que son extract, transform, load y visualize. La idea principal es lograr modularidad en el procesamiento de los datos. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Untitled diagram-2025-11-29-225514.png}{\centering}
    \caption{Diagrama general del sistema.}
\end{figure}

\subsubsection{Extract}

En el módulo de extract se implementó un diseño Template Method de forma a que la clase \texttt{BaseExtractor} sirve como una plantilla para subclases, es decir una estructura base. Donde \texttt{AbuseIPDBExtractor}, \texttt{URLHausExtractor} y \texttt{OTXExtractor} heredan sus métodos y propiedades para evitar redundancia ya que esta clase maneja los requests, los errores y el formato. Luego cada subclase genera un archivo JSON con los datos crudos que seguirán el flujo en la siguiente etapa.

\subsubsection{Transform}

El transform esta diseñado en capas, en la primera capa el módulo \texttt{normalizers.py} se encarga de convertir los archivos JSON en estructuras tipeadas utilizando funciones que cargan, mapean y convierten los registros al formato definido por la capa \texttt{models.py}, que define las estructura de los datos utilizando dataclasses en \texttt{AbuseIPDBRecord}, \texttt{URLHausRecord}, \texttt{OTXRecord} para los registros de cada fuente, y \texttt{CrossIndicatorRecord} para indicadores correlacionados. 

Finalmente \texttt{pipeline.py} se encarga de orquestar toda la transformación mediante clases que modularizan el trabajo como: \texttt{SourceRunner} que ejecuta los normalizadores, \texttt{CrossBuilder} que verifica indicadores iguales en las fuentes de datos, y si las encuentra consolida la información. \texttt{Serializer} convierte dataclass a diccionario y datetime a string, en \texttt{Writer} se almacenan los datos serializados en archivos JSON. Luego en la clase \texttt{TransformPipeline} se orquesta todo el flujo.

\subsubsection{Load}

En la clase \texttt{PostgresLoader} se crean las tablas \texttt{abuseipdb\_reports}, \texttt{urlhaus\_urls}, \texttt{otx\_indicators} y \texttt{cross\_indicators} si no existen, y se insertan los datos si no existen conflictos. También cada tabla tiene un campo JSONB que almacena el registro original.

\subsubsection{Visualize}

Grafana se conecta a la base de datos PostgreSQL como fuente de datos permitiendo crear visualizaciones interactivas de los datos obtenidos.

Esta estructura modulariza el sistema de forma que se puedan agregar nuevas fuentes de datos sin que afecte a todo el sistema, como también cambiar la base de datos sin afectar al transform o viceversa.

% \begin{figure}[H]
%     \centering
%     \fbox{\parbox{0.9\textwidth}{\centering
%     \vspace{0.3cm}
%     \textbf{EXTRACT} $\rightarrow$ \textbf{TRANSFORM} $\rightarrow$ \textbf{LOAD} $\rightarrow$ \textbf{VISUALIZE}\\[0.2cm]
%     \small{APIs Externas} \hspace{1cm} \small{Normalizacion} \hspace{1cm} \small{PostgreSQL} \hspace{1cm} \small{Grafana}
%     \vspace{0.3cm}
%     }}
%     \caption{Flujo de datos del pipeline ETL}
% \end{figure}

\subsection{Estructura del Proyecto}
El proyecto sigue una estructura modular que separa responsabilidades: comprendido por el extract, el transform, el load, pipelines que se encarga de ejecutar todo el proyecto, output que almacena temporalmente los datos obtenidos con los requests y los datos transformados. También creé un módulo que se llama task executor dedicado para archivos de dependencia de Apache Airflow y el DAG para la ejecución diaria del ETL. 
\begin{table}[H]
\centering
\scriptsize
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Directorio/Archivo} & \textbf{Descripción} \\
\midrule
\textbf{extract/} & \textbf{Módulo de extracción} \\
\hspace{0.3cm}\_\_init\_\_.py & Inicializador del paquete \\
\hspace{0.3cm}common/\_\_init\_\_.py, base.py, exceptions.py & Clase base y utilidades \\
\hspace{0.3cm}abuseipdb/\_\_init\_\_.py, extract\_abuseipdb.py & Extractor AbuseIPDB \\
\hspace{0.3cm}urlhaus/\_\_init\_\_.py, extract\_urlhaus.py & Extractor URLhaus \\
\hspace{0.3cm}otx/\_\_init\_\_.py, extract\_otx.py & Extractor OTX \\
\midrule
\textbf{transform/} & \textbf{Módulo de transformación} \\
\hspace{0.3cm}\_\_init\_\_.py, models.py, normalizers.py, pipeline.py & Normalización y dataclasses \\
\hspace{0.3cm}Dockerfile & Imagen Docker del transformador \\
\midrule
\textbf{load/} & \textbf{Módulo de carga} \\
\hspace{0.3cm}\_\_init\_\_.py, postgres\_loader.py & Clase PostgresLoader \\
\midrule
\textbf{pipelines/} & \textbf{Orquestación ETL} \\
\hspace{0.3cm}\_\_init\_\_.py, full\_pipeline.py & Pipeline principal \\
\midrule
\textbf{output/} & \textbf{Archivos generados} \\
\hspace{0.3cm}*\_data.json & Datos crudos (abuseipdb, urlhaus, otx) \\
\hspace{0.3cm}*\_records.json & Registros normalizados \\
\hspace{0.3cm}cross\_indicators.json & Correlación \\
\midrule
\textbf{task\_executor/airflow/} & \textbf{Automatización Airflow} \\
\hspace{0.3cm}dags/dark\_eye\_daily.py & DAG de ejecución diaria \\
\hspace{0.3cm}docker-compose.yaml, logs/, plugins/ & Configuración Airflow \\
\midrule
\textbf{Archivos raiz} & \\
\hspace{0.3cm}Dockerfile, docker-compose.yml & Empaquetamiento Docker \\
\hspace{0.3cm}requirements.txt, .env, README.md & Configuración y documentación \\
\bottomrule
\end{tabular}
\caption{Estructura del proyecto.}
\label{tab:estructura}
\end{table}
Y archivos que se encuentran en la raíz del proyecto que son los requirements.txt donde están todas las versiones de las librerías utilizadas, .env donde se encuentran todas las configuraciones del proyecto, el Dockerfile y docker-compose.yml que tienen la configuración necesaria para encapsular el proyecto en un contenedor y por último el README.md que tiene la toda la especificación del proyecto.

% ============================================================
\section{Pipeline ETL}
\subsection{Extract}
\subsubsection{Diseño del Módulo de Extracción}

El módulo de extracción sigue un diseño Template Method donde una clase puede servir de plantilla para subclasses de forma a evitar redundancia en el código. Para ello se creo extract/common/base.py que contiene la clase \texttt{BaseExtractor} que define una estructura común para todos los extractors el formato de configuración, la validación y el almacenamiento.

\begin{lstlisting}[language=Python, caption={Clase BaseExtractor}]
class BaseExtractor(ABC, Generic[TConfig]):

    logger_name: str = "extractors.base"
    result_filename: str = "data.json"
    
    def __init__(self, config: TConfig):
        self.config = config
        self.config.validate()
        self.logger = logging.getLogger(self.logger_name)
        self.config.output_dir.mkdir(parents=True, exist_ok=True)
        
    @abstractmethod
    def extract(self) -> List[dict]:
        """Cada extractor concreto debe implementar la descarga de datos."""
        raise NotImplementedError
        
    def save(self, data: List[dict]) -> Path:
        payload = {
            "timestamp": datetime.utcnow().isoformat(timespec="seconds"),
            "total": len(data),
            "data": data,
        }
        output = self.config.output_dir / self.result_filename
        with output.open("w", encoding="utf-8") as handle:
            json.dump(payload, handle, indent=2)
        self.logger.info("Resultados guardados en %s", output)
        return output
        
    def run(self) -> ExtractorResult:
        records = self.extract()
        output_path = self.save(records)
        return {
            "total": len(records),
            "output_path": str(output_path),
        }
\end{lstlisting}
Luego cada subclase hereda de \texttt{BaseExtractor} la estructura base e implementa la función \texttt{extract()} especifica para su fuente de datos.

\subsubsection{AbuseIPDB Extractor}
AbuseIPDB es un proyecto dedicado a combatir la propagación de hackers, spammers y actividades abusivas en internet. Su misión es contribuir a una web más segura proporcionando una lista negra centralizada para que webmasters, administradores de sistemas y otras partes interesadas puedan reportar y encontrar direcciones IP asociadas con actividades maliciosas en línea.

En la clase \texttt{AbuseIPDBExtractor} se hace un request de su API blacklist de IPs maliciosas reportadas.

\begin{lstlisting}[language=Python, caption={Extractor de AbuseIPDB}]
@dataclass(frozen=True)
class config_abuseipdb(BaseExtractorConfig):
    api_key: str = os.getenv("ABUSEIPDB_API_KEY", "")
    api_url: str = os.getenv("ABUSEIPDB_API_URL", "")
    limit: int = os.getenv("ABUSEIPDB_LIMIT", "")
    confidence_min: int = os.getenv("ABUSEIPDB_CONFIDENCE_MIN", "")

    def validate(self) -> None:
        super().validate()
        validate_required_fields(self, api_key="ABUSEIPDB_API_KEY", api_url="ABUSEIPDB_API_URL", limit="ABUSEIPDB_LIMIT", confidence_min="ABUSEIPDB_CONFIDENCE_MIN")

class AbuseIPDBExtractor(BaseExtractor[config_abuseipdb]):
    logger_name = "extractors.abuseipdb"
    result_filename = "abuseipdb_data.json"

    def extract(self) -> List[dict]:
        endpoint = self.config.api_url
        headers = {"Key": self.config.api_key,"Accept": "application/json",}
        params = {"limit": self.config.limit,"confidenceMinimum": self.config.confidence_min}
        response = safe_request(
            lambda: requests.get(endpoint, params=params, headers=headers, timeout=self.config.timeout),
            self.logger,
            retries=1,
            retry_delay=2,
        )
        return response.get("data", [])
\end{lstlisting}

Los datos extraídos incluyen: dirección IP, código de país, puntuación de confianza de abuso (0-100), numero de reportes distintos y timestamp del reporte mas reciente.

\subsubsection{URLhaus Extractor}

URLhaus es una plataforma de abuse.ch y Spamhaus dedicada a compartir URL maliciosas que se utilizan para la distribución de malware. Integra una API gratuita con reporte de URLs que se pueden utilizar para hacer consultas masivas.

En la clase \texttt{URLhausExtractor} se hace un request a su API de reportes de URLs que se utilizan para la distribución de malware. Donde cada registro incluye: URL completa, dominio o IP de hosting, estado actual (online/offline), tipo de amenaza asociada y tags describiendo la familia de malware.

\subsubsection{OTX Extractor}
LevelBlue Labs® Open Threat Exchange® (OTX™) es una plataforma de inteligencia de amenazas que ofrece acceso abierto a una comunidad global de investigadores de amenazas y profesionales de la seguridad. Ofrece datos de amenazas generados por la comunidad, facilitando la investigación colaborativa y automatizando el proceso de actualización de infraestructuras de seguridad con datos de amenazas de cualquier fuente. Para añadir riqueza a proyecto utilizamos la API que extrae específicamente indicadores y tipo de amenazas.

En la clase \texttt{OTXExtractor} se hace un request a su API de indicadores que incluye datos de: direcciones IP, dominios, URLs, hashes de archivos y direcciones de correo.

% ============================================================
\subsection{Transform}
En el módulo Transform se convierten los datos extraídos de las distintas fuentes de datos y se normalizan de forma a que sea consistente con la forma en la que se van a almacenar en la base de datos. 

En la Figura \ref{fig:transform} se visualiza la arquitectura detallada del Transform que sigue el siguiente flujo:

\textbf{1. Entrada de datos:} Se reciben los archivos JSON generados por los extractores: \texttt{abuseipdb\_data.json}, \texttt{urlhaus\_data.json} y \texttt{otx\_data.json}.

\textbf{2. Normalizers:} Consiste en funciones que procesan cada fuente de datos y mapea los campos de cada API a un formato estándar. Por ejemplo, \texttt{transform\_abuseipdb} convierte el campo \texttt{ipAddress} a \texttt{ip\_address} y parsea las fechas a formato ISO 8601.

\textbf{3. Dataclasses:} Los datos normalizados se almacenan en estructuras tipadas:
\begin{itemize}
    \item \texttt{AbuseIPDBRecord}: IP, código de país, puntuación de abuso, timestamp.
    \item \texttt{URLHausRecord}: URL, host, estado, tipo de amenaza, tags.
    \item \texttt{OTXRecord}: Indicador, tipo, titulo, descripción.
    \item \texttt{CrossIndicatorRecord}: Indicadores detectados en múltiples fuentes.
\end{itemize}

\textbf{4. CrossBuilder:} Es una clase que analiza las distintas fuentes para identificar si hay algún indicador igual en ellas. Por ejemplo, si se identifica una IP en AbuseIPDB y en OTX, se genera un \texttt{CrossIndicatorRecord} que consolida la información, aumentando la confianza en la amenaza detectada.

\textbf{5. Writer:} Se serializan los registros normalizados y se almacenan en archivos JSON (\texttt{*\_records.json} y \texttt{cross\_indicators.json}). 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{Untitled diagram-2025-11-30-151819.png}{\centering}
\caption{Diagrama general de Transform.}
\label{fig:transform}
\end{figure}

\subsubsection{Modelos de Datos}

La estructura normalizada para cada tipo de registro se define en transform/models.py mediante dataclases. 

En \texttt{raw} se almacena el registro original completo, para contar con los campos no normalizados.

\begin{lstlisting}[language=Python, caption={Modelos de datos normalizados}]
@dataclass(slots=True)
class AbuseIPDBRecord:
    ip_address: str
    country_code: str | None
    abuse_confidence: int | None
    last_reported_at: datetime | None
    raw: dict[str, Any] = field(default_factory=dict)

@dataclass(slots=True)
class URLHausRecord:
    url_id: int
    url: str
    url_status: str | None
    host: str | None
    date_added: datetime | None
    threat: str | None
    urlhaus_reference: str | None
    tags: list[str] = field(default_factory=list)
    raw: dict[str, Any] = field(default_factory=dict)

@dataclass(slots=True)
class OTXRecord:
    pulse_id: int
    indicator: str
    indicator_type: str
    title: str | None
    description: str | None
    content: str | None
    raw: dict[str, Any] = field(default_factory=dict)

@dataclass(slots=True)
class CrossIndicatorRecord:
    indicator: str
    indicator_type: str
    sources: list[str]
    tags: list[str]
    first_seen: datetime | None
    last_seen: datetime | None
\end{lstlisting}

\subsubsection{Funciones Normalizadoras}

Para cada fuente de datos se diseño una función normalizadora personalizada que maneja el formato de cada API en específica. Las funciones diseñadas son: \texttt{\_abuse}, \texttt{\_urlhaus} y \texttt{\_otx}.

Estos normalizadores también almacenan los datos originales dentro de cada registro en el campo \texttt{raw}, permitiendo acceso a campos específicos de la fuente.

\clearpage
\begin{lstlisting}[language=Python, caption={Normalizador de AbuseIPDB}]
def _abuse(row: dict, fallback: datetime | None) -> AbuseIPDBRecord:
    last = row.get("lastReportedAt")
    last_dt = datetime.fromisoformat(last.replace("Z", "+00:00")) if last else fallback
    return AbuseIPDBRecord(
        ip_address=row["ipAddress"],
        country_code=row.get("countryCode"),
        abuse_confidence=int(row.get("abuseConfidenceScore") or 0),
        last_reported_at=last_dt,
        raw=row,
    )
\end{lstlisting}

\subsubsection{Transform Pipeline}

En módulo transform se diseño de forma a que en \texttt{transform/pipeline.py} la clase \texttt{TransformPipeline} orqueste el flujo de la transformación llamando a las clases del Cuadro \ref{tab:transform-classes}.

\begin{table}[H]
\centering
\begin{tabular}{@{}lp{8cm}@{}}
\toprule
\textbf{Clase} & \textbf{Responsabilidad} \\
\midrule
SourceConfig & Configura cada fuente: nombre, archivo JSON y funcion normalizadora. \\
SourceRunner & Ejecuta la normalizacion de cada fuente y retorna los registros procesados. \\
CrossBuilder & Detecta indicadores que aparecen en multiples fuentes y los agrupa. \\
Serializer & Convierte dataclasses a diccionarios y formatea fechas a ISO 8601. \\
Writer & Guarda los resultados en archivos JSON. \\
\bottomrule
\end{tabular}
\caption{Clases del módulo Transform}
\label{tab:transform-classes}
\end{table}

\begin{lstlisting}[language=Python, caption={Pipeline de transformación}]
class TransformPipeline:
    def run(self) -> dict[str, Any]:
        per_source = self.runner.run()          
        cross = self.cross_builder.build(per_source)  
        self.writer.write_sources(per_source)    
        self.writer.write_cross(cross)           
        return {"sources": per_source, "cross": cross}
\end{lstlisting}

El pipeline se resume en: normalizar los datos de cada fuente, identificar identificadores IPs que aparecen en mas de una fuente de datos y escribir los resultados normalizados en archivos JSON que seguirán el flujo en el módulo Load.

% ============================================================
\subsection{Load}

\subsubsection{PostgresLoader}

En load/postgres\_loader.py se define una clase \texttt{PostgresLoader} que se encarga de todas las operaciones de base de datos utilizando psycopg3 para conectarse a PostgreSQL.

\begin{lstlisting}[language=Python, caption={Inicializacion de esquema}]
def _init_schema(self, conn: psycopg.Connection) -> None:
    conn.execute("""
        CREATE TABLE IF NOT EXISTS abuseipdb_reports (
            ip_address TEXT PRIMARY KEY,
            country_code TEXT,
            abuse_confidence INTEGER,
            last_reported_at TIMESTAMPTZ,
            raw JSONB
        )
        """
    )
    conn.execute("""
        CREATE TABLE IF NOT EXISTS urlhaus_urls (
            url_id BIGINT PRIMARY KEY,
            url TEXT NOT NULL,
            url_status TEXT,
            host TEXT,
            date_added TIMESTAMPTZ,
            threat TEXT,
            urlhaus_reference TEXT,
            tags TEXT[],
            raw JSONB
        )
        """
    )
    conn.execute("""
        CREATE TABLE IF NOT EXISTS otx_indicators (
            pulse_id BIGINT PRIMARY KEY,
            indicator TEXT NOT NULL,
            indicator_type TEXT NOT NULL,
            title TEXT,
            description TEXT,
            content TEXT,
            raw JSONB
        )
        """
    )
    conn.execute("""
        CREATE TABLE IF NOT EXISTS cross_indicators (
            indicator TEXT NOT NULL,
            indicator_type TEXT NOT NULL,
            sources TEXT[] NOT NULL,
            tags TEXT[],
            first_seen TIMESTAMPTZ,
            last_seen TIMESTAMPTZ,
            PRIMARY KEY (indicator, indicator_type)
        )
        """
    )
\end{lstlisting}

\subsubsection{Método Load}

Cada fuente de datos tiene su propio método load que intenta insertar un nuevo registro con los campos del dataclass si no hay conflicto, si ya existe un registro igual se ejecuta DO UPDATE. Estos métodos aseguran que no haya registros duplicados y que siempre se mantenga el registro más actualizado.

\begin{lstlisting}[language=Python, caption={Carga de registros AbuseIPDB}]
def _load_abuseipdb(self, conn: psycopg.Connection, records: Iterable[AbuseIPDBRecord]) -> int:
    total = 0
    for rec in records:
        conn.execute(
            """
            INSERT INTO abuseipdb_reports (
                ip_address, country_code, abuse_confidence,
                last_reported_at, raw
            )
            VALUES (%s, %s, %s, %s, %s)
            ON CONFLICT (ip_address) DO UPDATE SET
                country_code = EXCLUDED.country_code,
                abuse_confidence = EXCLUDED.abuse_confidence,
                last_reported_at = GREATEST(
                    abuseipdb_reports.last_reported_at,
                    EXCLUDED.last_reported_at
                ),
                raw = EXCLUDED.raw
            """,
            (rec.ip_address, rec.country_code, rec.abuse_confidence,
             rec.last_reported_at, self._json(asdict(rec))),
        )
        total += 1
    return total
\end{lstlisting}

% ============================================================
\subsection{Orquestación del Pipeline}

En \texttt{pipelines/full\_pipeline.py} se orquesta la ejecución completa del flujo ETL, conectando los módulos Extract, Transform y Load en secuencia.

\begin{lstlisting}[language=Python, caption={Pipeline principal}]
EXTRACTORS: list[tuple[str, Callable[[], object]]] = [
    ("urlhaus", lambda: URLHausExtractor(config_urlhaus())),
    ("abuseipdb", lambda: AbuseIPDBExtractor(config_abuseipdb())),
    ("otx", lambda: OTXExtractor(config_otx())),
]

def run(output_dir: Path = Path("output"), run_extract: bool = True) -> None:
    if run_extract:
        for name, factory in EXTRACTORS:
            extractor = factory()
            result = extractor.run()
    
    result = run_transform(output_dir)

    loader = PostgresLoader(dsn=os.environ["DATABASE_URL"])
    loader.load(
        abuseipdb=result["sources"]["abuseipdb"],
        urlhaus=result["sources"]["urlhaus"],
        otx=result["sources"]["otx"],
        cross=result["cross"],
    )
\end{lstlisting}

Este módulo es llamado por el Dockerfile para ejecutar el proyecto como en Listing \ref{lis:ejec_etl} y y por el DAG de Apache Airflow para la ejecución programada.

\begin{lstlisting}[language=Dockerfile, caption={Parte del Dockerfile que ejecuta el ETL.}, label={lis:ejec_etl}]
CMD ["python", "-m", "pipelines.full_pipeline"]
\end{lstlisting}

\section{Empaquetado con Docker}

El proyecto se empaqueta utilizando Docker una plataforma open source que nos permite empaquetar nuestra aplicación con todas sus dependencias en contenedores. 

Estos contenedores garantizan que el software funcione de manera consistente en cualquier entorno, ya sea local, en la nube o en otros servidores. Esta visualización a nivel de sistema operativo es más eficiente que las máquinas virtuales tradicionales porque los contenedores comparten el mismo kernel del sistema operativo.  

El empaquetado con Docker nos garantiza:

\begin{itemize}
    \item \textbf{Reproducibilidad:} Ya que Docker encapsula la aplicación y sus dependencias.
        
    \item \textbf{Aislamiento:} El contenedor corre de forma aislada del sistema operativo de la máquina.
        
    \item \textbf{Portabilidad:} La imagen Docker puede ejecutarse en cualquier plataforma que soporte Docker: Windows, Linux, macOS, servidores cloud (AWS, Azure, GCP) o clústers Kubernetes.
        
    \item \textbf{Versionado de entornos:} Ya que las imágenes Docker se etiquetan con versiones.
\end{itemize}

Para este proyecto es útil porque depende de servicios externos como PostgreSQL, Grafana y Apache Airflow que deben estar disponibles y configurados correctamente antes de ejecutar el ETL.

\subsection{Elección de Imagen Base}

\begin{table}[H]
\centering
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Imagen} & \textbf{Tamano} & \textbf{Paquetes} & \textbf{Soporte} \\
\midrule
python:3.11        & 920 MB & Completo  & Oficial \\
python:3.11-slim   & 130 MB & Minimo    & Oficial \\
python:3.11-alpine & 50 MB  & Muy minimo & Comunidad \\
\bottomrule
\end{tabular}
\caption{Comparación de imágenes base de Docker}

\end{table}

Se seleccionó \textbf{python:3.11-slim} porque:
\begin{itemize}
    \item Tiene compatibilidad con psycopg: basada en Debian, acceso directo a libpq-dev.
    \item El tamaño es de solo 130MB.
    \item Soporte oficial de Python Software Foundation.
\end{itemize}

\subsection{Dockerfile Completo}

\begin{lstlisting}[language=Dockerfile, caption={Dockerfile del proyecto}]
FROM python:3.11-slim

ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1

RUN apt-get update \
 && apt-get install -y --no-install-recommends build-essential libpq-dev \
 && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "-m", "pipelines.full_pipeline"]
\end{lstlisting}

\subsection{Análisis del Dockerfile}

\begin{table}[H]
\centering
\begin{tabular}{@{}lp{9cm}@{}}
\toprule
\textbf{Instrucción} & \textbf{Propósito} \\
\midrule
FROM python:3.11-slim & Imagen base con Python 3.11 sobre Debian \\
ENV & Variables para optimizar Python en contenedores \\
RUN apt-get & Instala compiladores y headers de PostgreSQL \\
WORKDIR /app & Define directorio de trabajo \\
COPY + RUN pip & Instala dependencias Python con caché optimizado \\
CMD & Comando de ejecución del pipeline \\
\bottomrule
\end{tabular}
\caption{Instrucciones del Dockerfile}
\end{table}

\subsection{Dependencias Python}

\begin{table}[H]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Librería} & \textbf{Propósito} \\
\midrule
python-dotenv & Carga variables de entorno desde .env \\
requests & Cliente HTTP para las APIs externas \\
psycopg[binary] & Driver de PostgreSQL \\
\bottomrule
\end{tabular}
\caption{Librerías Python}
\end{table}

% ============================================================
\section{Orquestación con Docker Compose}
Docker Compose se encarga de orquestar contenedores que trabajan juntos. En este proyecto gestiona tres servicios que se comunican a través de una red Docker con con resolución DNS.
%\subsection{Arquitectura de Servicios}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Untitled diagram-2025-11-30-180838.png}{\centering}
    \caption{Diagrama general del docker-compose.yml de dark\_eye\_core}
\end{figure}

\subsection{Servicio PostgreSQL}

\begin{lstlisting}[language=yaml, caption={Configuracion de PostgreSQL}]
postgres:
  image: postgres:15
  container_name: dark_eye_postgres
  environment:
    POSTGRES_DB: ${POSTGRES_DB}
    POSTGRES_USER: ${POSTGRES_USER}
    POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
  volumes:
    - pgdata:/var/lib/postgresql/data
  ports:
    - "5433:5432"
  healthcheck:
    test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
    interval: 10s
    timeout: 5s
    retries: 5
\end{lstlisting}

\subsection{Servicio Pipeline}

\begin{lstlisting}[language=yaml, caption={Configuracion del Pipeline}]
pipeline:
  build: .
  container_name: dark_eye_pipeline
  env_file:
    - .env
  depends_on:
    postgres:
      condition: service_healthy
  volumes:
    - ./output:/app/output
  environment:
    DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
\end{lstlisting}

\subsection{Automatización con Apache Airflow}

El proyecto también tiene una configuración de Docker Compose para Apache Airflow en \texttt{task\_executor/airflow/}, que permite programar la ejecución automática del pipeline.

\begin{lstlisting}[language=yaml, caption={Docker Compose de Airflow (extracto)}]
# task_executor/airflow/docker-compose.yaml
services:
  airflow-webserver:
    image: apache/airflow:2.7.0
    # ...
  airflow-scheduler:
    image: apache/airflow:2.7.0
    # ...
\end{lstlisting}

Para la ejecución diaria del pipeline ETL se diseñó el DAG \texttt{dark\_eye\_daily.py} que programa la ejecución diaria 03:00 UTC, de forma que los datos se actualicen automáticamente y puedan ser visibles en Grafana.     


\section{Visualización con Grafana}

Grafana es un software de código abierto que permite crear paneles, realizar consultar, visualizaciones específicas, generar alertas, explorar métricas y registros. 

En este proyecto se conecta Grafana a nuestra base de datos PostgreSQL para hacer visible en un panel los datos de indicadores amenazantes que hay en internet.

\subsection{Configuración del Servicio}
\begin{lstlisting}[language=yaml, caption={Configuracion de Grafana}]
grafana:
  image: grafana/grafana:latest
  container_name: dark_eye_grafana
  ports:
    - "3000:3000"
  environment:
    - GF_SECURITY_ADMIN_USER=${GRAFANA_USER:-admin}
    - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
    - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource
  volumes:
    - grafana_data:/var/lib/grafana
  depends_on:
    postgres:
      condition: service_healthy
  restart: unless-stopped
\end{lstlisting}
En el archivo docker-compose-yml se configura un servicio para Grafana con los siguientes parámetros:
\begin{itemize}
    \item \textbf{image:} Se configura con la versión más reciente de la imagen oficial de Grafana.
    
    \item \textbf{ports:} Se expone el puerto 3000, permitiendo acceder a la interfaz web desde \url{http://localhost:3000}.
    
    \item \textbf{environment:} Se configuran las credenciales con las variables almacenadas en el archivo \texttt{.env}.
    
    \item \textbf{volumes:} Reescribe la configuración de Grafana en un volumen nombrado.
        
    \item \textbf{depends\_on:} Espera a que PostgreSQL este \texttt{healthy} antes de iniciar, asegurando que la base de datos este disponible.
    
    \item \textbf{restart:} Reinicia automáticamente el servicio si falla o si se reinicia el servidor.
\end{itemize}

\subsection{Conexión a PostgreSQL}

Para configurar la fuente de datos en Grafana se siguieron los siguientes pasos:
\begin{enumerate}
    \item Se accede a \url{http://localhost:3000}
    \item Se navega a Connection $\rightarrow$ Data Sources
    \item Se agrega la fuente de datos PostgreSQL con los parametros:
    \begin{itemize}
        \item Host: \texttt{postgres:5432}
        \item Database: nombre configurado en .env
        \item User/Password: credenciales del .env
        \item SSL Mode: disable (red interna Docker)
    \end{itemize}
\end{enumerate}

\subsection{Visualización Dark Eye Core}

Los paneles de visualización Dark Eye Core están organizados en cuatro secciones; la primera es el core donde se muestra un vistazo general del proyecto y las siguientes tres secciones se organizan por cada fuente de datos.

\subsubsection{Dark Eye Core}

En esta sección se visualiza un gráfico de barras que muestra la cantidad de datos extraídos por API y las IPs coincidentes detectadas. Y una tabla que clasifica los indicadores por fuente, y la cantidad de los mismos.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.82\linewidth]{panel_general.png}
    \caption{Visualización general Dark Eye Core}
    \label{fig:placeholder}
\end{figure}

\subsubsection{AbuseIPDB}

En esta sección se visualiza un gráfico de barras horizontal que muestra la cantidad de IPs reportadas por país y el top de IPs más reportadas incluyendo país, nivel de confianza (abuse\_confidence), fecha del último reporte y cantidad de reportes.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.82\linewidth]{abuse.png}
    \caption{Panel de visualización de AbuseIPDB}
    \label{fig:placeholder}
\end{figure}
\subsubsection{URLhaus}

En la sección de URLhaus se visualiza un gráfico circular que segmenta el estado de las URLs; las que están activas y las que no. Y una tabla con los hosts más frecuentes, cantidad de URLs asociadas, tipos de amenaza y fecha de última detección.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.82\linewidth]{urlhaus.png}
    \caption{Panel de visualización de URLhaus}
    \label{fig:placeholder}
\end{figure}

\subsubsection{OTX}

En esta sección se visualiza un gráfico de barras que clasifica los indicadores por tipo y una tabla con los indicadores mas maliciosos y su clasificación por tipo.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.82\linewidth]{otx.png}
    \caption{Panel de visualización de OTX}
    \label{fig:placeholder}
\end{figure}

% ============================================================
\section{Despliegue y Ejecución}

El despliegue y la ejecución del proyecto tiene dos configuraciones principales de Docker Compose: una para los servicios principales (PostgreSQL, Pipeline, Grafana) y otra para Apache Airflow que automatiza la ejecución diaria. En la Figura \ref{fig:diagrama_completo}
se visualiza el diagrama completo del proyecto.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\linewidth]{diagrama_completo.png}
    \caption{Diagrama completo del sistema}
    \label{fig:diagrama_completo}
\end{figure}

\subsection{Comandos de Despliegue}

Para desplegar los servicios principales:
\begin{lstlisting}[language=bash]
#Construir imagen
docker build -t dark-eye-pipeline:latest .
#Iniciar servicios principales
docker compose up -d
\end{lstlisting}

Para desplegar Apache Airflow:
\begin{lstlisting}[language=bash]
#Desde task_executor/airflow/
cd task_executor/airflow
docker compose up -d
\end{lstlisting}

\subsection{Evidencias de Ejecución}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.82\linewidth]{build_dark_eye.png}
    \caption{Construcción de la imagen Docker del pipeline}
    \label{fig:build}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.82\linewidth]{pruebasDocker.png}
    \caption{Servicios de Docker Compose en ejecución}
    \label{fig:docker-ps}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.82\linewidth]{logs_pipeline.png}
    \caption{Logs del pipeline ETL mostrando extracción, transformacion y carga}
    \label{fig:logs}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.82\linewidth]{ejecucion_airflow.png}
    \caption{Servicios del Docker Compose de Apache Airflow en ejecución}
    \label{fig:placeholder}
\end{figure}

\subsection{Verificación del Sistema}

Se verifica que las tablas hayan sido creadas correctamente utilizando la terminal y consultando a la base de datos creada. También se utilizó DBeaver para la visualización y el control de la base de datos, y la plataforma web de Apache Airflow para administrar el DAG.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.82\linewidth]{tablas_creadas.png}
    \caption{Consultas a la base de datos desde la terminal}
    \label{fig:placeholder}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.82\linewidth]{dbeaver.png}
    \caption{Visualización de las tablas en DBeaver}
    \label{fig:placeholder}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.82\linewidth]{plataforma_apache_airflow.png}
    \caption{Visualización del DAG programado en Apache Airflow}
    \label{fig:placeholder}
\end{figure}


% ============================================================
\section{Resultados}
A continuación se muestran capturas de la infraestructura Docker desplegada en la plataforma.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.82\linewidth]{volumen.png}
    \caption{Volúmenes de Docker para persistencia de datos}
    \label{fig:volumen}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.82\linewidth]{imagenDocker.png}
    \caption{Imagen Docker de Dark Eye Core de 882.94 MB}
    \label{fig:imagen}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.82\linewidth]{container_airflow.png}
    \caption{Contenedores de Apache Airflow en ejecución}
    \label{fig:airflow}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.82\linewidth]{container_dark_eye_core.png}
    \caption{Contenedores Dark Eye Core}
    \label{fig:containers}
\end{figure}
\section{Conclusiones}
En este proyecto se logró implementar un sistema modular para extraer, transformar, cargar y visualizar datos de tres fuentes distintas AbuseIPDB, URLhaus y OTX que proporcionan información de indicadores de amenazas en internet. 

Con la implementación de docker se logró una aplicación empaquetada que garantiza un sistema aislado y reproducible en cualquier entorno. Con despliegue de Apache Airflow se automatizó la ejecución diaria del panel Dark Eye Core, que fueron desarrollados en Grafana con conexión a una base de datos PostgreSQL.

Finalmente la implantación de este proyecto logró hacer visible indicadores de amenazas que hay en internet conectando diversas fuentes de datos. La arquitectura fue diseñada de forma modular de forma a que en el futuro se pueda expandir el sistema agregando nuevas fuentes de datos o modificando componentes individuales sin afectar al resto del sistema.

% ============================================================
\begin{thebibliography}{99}
  \bibitem{docker} Docker Inc. \textit{Docker Documentation}. \url{https://docs.docker.com/}
  \bibitem{python} Python Software Foundation. \textit{Python Docker Image}. \url{https://hub.docker.com/_/python}
  \bibitem{abuseipdb} AbuseIPDB. \textit{API Documentation}. \url{https://docs.abuseipdb.com/}
  \bibitem{urlhaus} abuse.ch. \textit{URLhaus API}. \url{https://urlhaus.abuse.ch/api/}
  \bibitem{otx} AlienVault. \textit{OTX API}. \url{https://otx.alienvault.com/api}
  \bibitem{grafana} Grafana Labs. \textit{Grafana Documentation}. \url{https://grafana.com/docs/}
  \bibitem{psycopg} Psycopg Team. \textit{Psycopg 3}. \url{https://www.psycopg.org/psycopg3/docs/}
\end{thebibliography}

\end{document}