\documentclass[12pt, a4paper]{article}

% ==================== CODIFICACION Y LENGUAJE ====================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[spanish]{babel}

% ==================== GEOMETRIA Y FORMATO ====================
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{parskip}
\usepackage{setspace}
\onehalfspacing

% ==================== TIPOGRAFIA ====================
\usepackage{lmodern}

% ==================== MATEMATICAS ====================
\usepackage{amsmath, amssymb}

% ==================== GRAFICOS Y FIGURAS ====================
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}

% ==================== TABLAS ====================
\usepackage{array, booktabs}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{multirow}

% ==================== COLORES ====================
\usepackage[dvipsnames, table]{xcolor}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.98, 0.98, 0.98}
\definecolor{accentblue}{HTML}{2563EB}
\definecolor{accentgreen}{HTML}{059669}
\definecolor{accentorange}{HTML}{D97706}
\definecolor{lightblue}{HTML}{EFF6FF}
\definecolor{lightgreen}{HTML}{ECFDF5}
\definecolor{lightorange}{HTML}{FFFBEB}
\definecolor{sectioncolor}{HTML}{1E40AF}

% ==================== CAJAS DESTACADAS ====================
\usepackage[most]{tcolorbox}

\newtcolorbox{keyinsight}[1][]{
    colback=lightblue,
    colframe=accentblue,
    coltitle=white,
    fonttitle=\bfseries,
    title=#1,
    rounded corners,
    boxrule=0.8pt,
    left=6pt, right=6pt, top=4pt, bottom=4pt,
}

\newtcolorbox{metricbox}[1][]{
    colback=lightgreen,
    colframe=accentgreen,
    coltitle=white,
    fonttitle=\bfseries,
    title=#1,
    rounded corners,
    boxrule=0.8pt,
    left=6pt, right=6pt, top=4pt, bottom=4pt,
}

\newtcolorbox{warningbox}[1][]{
    colback=lightorange,
    colframe=accentorange,
    coltitle=white,
    fonttitle=\bfseries,
    title=#1,
    rounded corners,
    boxrule=0.8pt,
    left=6pt, right=6pt, top=4pt, bottom=4pt,
}

% ==================== DIAGRAMAS ====================
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, calc, fit, backgrounds}

% ==================== CODIGO FUENTE ====================
\usepackage{listings}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue!80!black}\bfseries,
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    frame=single,
    xleftmargin=2em,
    framexleftmargin=1.5em,
    rulecolor=\color{codegray!40},
}
\lstset{style=mystyle}

\lstdefinelanguage{SQL}{
    keywords={CREATE, TABLE, IF, NOT, EXISTS, PRIMARY, KEY, FOREIGN, REFERENCES, DEFAULT, ON, INDEX, PRAGMA, INTEGER, TEXT, UNIQUE, INSERT, INTO, VALUES, CONFLICT, DO, UPDATE, SET},
    keywordstyle=\color{blue!80!black}\bfseries,
    sensitive=false,
    comment=[l]{--},
    commentstyle=\color{codegreen}\ttfamily,
    stringstyle=\color{codepurple}\ttfamily,
    morestring=[b]',
    morestring=[b]"
}

% ==================== ENLACES ====================
\usepackage[hidelinks, colorlinks=true, linkcolor=accentblue, urlcolor=accentblue, citecolor=accentblue]{hyperref}

% ==================== ENCABEZADOS ====================
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small\leftmark}
\fancyhead[R]{\small\textcolor{accentblue}{Moltbook Karma}}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% ==================== SECCIONES COLOREADAS ====================
\usepackage{titlesec}
\titleformat{\section}
    {\Large\bfseries\color{sectioncolor}}
    {\thesection}{1em}{}
\titleformat{\subsection}
    {\large\bfseries\color{sectioncolor!80}}
    {\thesubsection}{1em}{}

% ==================== DOCUMENTO ====================
\begin{document}

% ---------- PORTADA ----------
\begin{titlepage}
    \centering
    \vspace*{0.5cm}

    \includegraphics[width=5.5cm]{LaSalleBCN.png}

    \vspace{1cm}

    {\scshape\large M\'aster Universitario en Ciencia de Datos\\[0.2cm]
    Universitat Ramon Llull -- La Salle\par}

    \vspace{1.5cm}

    \rule{\textwidth}{1.5pt}\\[0.4cm]
    {\LARGE\bfseries Pipeline de Ingenier\'ia de Datos\\para Predicci\'on de Karma\par}
    \vspace{0.3cm}

    \includegraphics[width=6cm]{assets/banner.jpg}

    \vspace{0.3cm}
    {\large\itshape Moltbook Karma\par}
    \rule{\textwidth}{1.5pt}

    \vspace{1.5cm}

    {\large MD003 -- Estructuras de datos y su almacenamiento\par}

    \vfill

    {\large
    Paulina Peralta \\[0.4cm]
    14 de febrero de 2026
    }

\end{titlepage}

% ---------- RESUMEN ----------
\begin{abstract}
\noindent
Este documento presenta el dise\~no e implementaci\'on de un pipeline de ingenier\'ia de datos completo para la predicci\'on de karma en \texttt{moltbook.com}, una red social donde agentes aut\'onomos de inteligencia artificial interact\'uan entre s\'i. El sistema abarca web scraping con Playwright para contenido JavaScript din\'amico, almacenamiento en SQLite con esquema normalizado, procesamiento en arquitectura Medallion (Silver/Gold) con Polars Lazy evaluation, an\'alisis exploratorio con PySpark, y entrenamiento de un modelo de regresi\'on con H2O AutoML. El modelo GBM resultante alcanza un \textbf{R$^2$ = 0.6363} y un \textbf{MAE = 3,234.81} sobre un dataset de 981 usuarios, 1,242 publicaciones y 644 comentarios. Se presenta la arquitectura modular del sistema, las decisiones de dise\~no, y el an\'alisis detallado de resultados.
\end{abstract}

\clearpage

% ---------- INDICE ----------
\tableofcontents
\clearpage

\listoffigures
\listoftables
\clearpage

% ============================================================
\section{Introducci\'on}

\subsection{Contexto del Proyecto}

Moltbook Karma es un proyecto de ingenier\'ia de datos que tiene como objetivo extraer, procesar y modelar datos de \texttt{moltbook.com}, una red social innovadora donde agentes aut\'onomos de inteligencia artificial interact\'uan entre s\'i: publican contenido, comentan, votan y acumulan una puntuaci\'on de reputaci\'on denominada \textbf{karma}.

\begin{keyinsight}[Sobre Moltbook.com]
Moltbook.com es una plataforma que simula una red social operada exclusivamente por agentes de IA. Estos agentes crean perfiles, publican contenido, comentan y votan, generando una econom\'ia de reputaci\'on medida en \textit{karma}. El sitio fue creado en el contexto de investigaci\'on sobre seguridad y comportamiento de agentes aut\'onomos de IA.
\end{keyinsight}

Este entorno presenta un desaf\'io t\'ecnico interesante: al tratarse de una Single Page Application (SPA) renderizada con JavaScript, las t\'ecnicas convencionales de scraping basadas en requests HTTP no son suficientes. Se requiere automatizaci\'on de navegador con Playwright para renderizar el contenido din\'amico antes de poder extraerlo.

El proyecto implementa un pipeline completo de datos que abarca desde la recolecci\'on mediante web scraping, el almacenamiento en una base de datos relacional SQLite, el procesamiento en capas siguiendo la arquitectura Medallion (Silver/Gold), hasta el entrenamiento de un modelo de regresi\'on con H2O AutoML para predecir el karma de los usuarios a partir de sus caracter\'isticas de actividad e interacci\'on en la plataforma.

\subsection{Objetivos}

Los objetivos principales de este proyecto son:

\begin{enumerate}
    \item Dise\~nar un scraper robusto con Playwright para extraer datos de moltbook.com, con rate limiting, reintentos y cach\'e HTML.
    \item Almacenar los datos crudos en una base de datos SQLite con un esquema relacional normalizado con foreign keys e \'indices.
    \item Implementar una arquitectura Medallion (Silver/Gold) con Polars utilizando evaluaci\'on lazy para eficiencia en memoria.
    \item Realizar an\'alisis exploratorio de datos (EDA) con PySpark para entender la distribuci\'on del karma y las correlaciones.
    \item Entrenar un modelo de regresi\'on con H2O AutoML para predecir el karma de los usuarios y optimizarlo con Grid Search.
    \item Construir una CLI profesional con Click para orquestar todas las etapas del pipeline.
\end{enumerate}

\subsection{Alcance}

El dataset recopilado comprende:

\begin{table}[H]
\centering
\begin{tabular}{@{}lr@{}}
\toprule
\textbf{Entidad} & \textbf{Cantidad} \\
\midrule
Usuarios & 981 \\
Publicaciones & 1,242 \\
Comentarios & 644 \\
Comunidades (SubMolts) & 55 \\
\bottomrule
\end{tabular}
\caption{Resumen del dataset recopilado.}
\end{table}

El scraping se realiz\'o respetando pr\'acticas \'eticas: rate limiting de 1 solicitud por segundo, identificaci\'on mediante User-Agent, y cach\'e HTML para evitar solicitudes redundantes.

% ============================================================
\clearpage
\section{Arquitectura del Sistema}

\subsection{Visi\'on General del Pipeline}

El pipeline sigue un flujo lineal de seis etapas que transforma datos crudos de la web en predicciones de karma:

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        node distance=0.8cm,
        stage/.style={
            rectangle, rounded corners=4pt,
            minimum width=2.2cm, minimum height=1.4cm,
            text centered, font=\small\bfseries,
            text=white, drop shadow,
        },
        arrow/.style={-{Stealth[length=3mm]}, thick, color=codegray},
        label/.style={font=\tiny, color=codegray, text centered},
    ]
        \node[stage, fill=accentblue] (scraping) {Scraping};
        \node[stage, fill=accentblue!80, right=of scraping] (sqlite) {SQLite};
        \node[stage, fill=accentgreen!90, right=of sqlite] (silver) {Silver};
        \node[stage, fill=accentgreen, right=of silver] (gold) {Gold};
        \node[stage, fill=accentorange!90, right=of gold] (modelo) {Modelo};
        \node[stage, fill=accentorange, right=of modelo] (eval) {Eval};

        \draw[arrow] (scraping) -- (sqlite);
        \draw[arrow] (sqlite) -- (silver);
        \draw[arrow] (silver) -- (gold);
        \draw[arrow] (gold) -- (modelo);
        \draw[arrow] (modelo) -- (eval);

        \node[label, below=0.15cm of scraping] {Playwright};
        \node[label, below=0.15cm of sqlite] {Base de datos};
        \node[label, below=0.15cm of silver] {Polars Lazy};
        \node[label, below=0.15cm of gold] {Features};
        \node[label, below=0.15cm of modelo] {H2O AutoML};
        \node[label, below=0.15cm of eval] {PySpark};
    \end{tikzpicture}
    \caption{Flujo general del pipeline de datos.}
    \label{fig:pipeline}
\end{figure}

% Placeholder para diagrama de arquitectura completo
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.9\textwidth]{diagrama_arquitectura.png}
%     \caption{Diagrama de arquitectura del sistema.}
% \end{figure}

La arquitectura est\'a dise\~nada de forma modular, donde cada etapa es independiente y puede ejecutarse por separado. La separaci\'on de responsabilidades permite modificar una etapa sin afectar a las dem\'as.

\subsection{Stack Tecnol\'ogico}

\begin{table}[H]
\centering
\scriptsize
\begin{tabular}{@{}llp{7cm}@{}}
\toprule
\textbf{Componente} & \textbf{Tecnolog\'ia} & \textbf{Prop\'osito} \\
\midrule
Web Scraping & Playwright + BeautifulSoup4 & Renderizado JavaScript + parsing HTML \\
Base de Datos & SQLite & Almacenamiento relacional local \\
Procesamiento & Polars (Lazy evaluation) & Limpieza y feature engineering eficiente \\
An\'alisis & PySpark & EDA y evaluaci\'on distribuida \\
Machine Learning & H2O AutoML & Regresi\'on automatizada con selecci\'on de modelos \\
Configuraci\'on & Pydantic Settings & Validaci\'on tipada de configuraci\'on \\
CLI & Click & Interfaz de l\'inea de comandos \\
Testing & pytest & Pruebas unitarias \\
Serializaci\'on & PyArrow / Parquet & Formato columnar eficiente \\
Reintentos & Tenacity & Backoff exponencial ante fallos \\
\bottomrule
\end{tabular}
\caption{Stack tecnol\'ogico del proyecto.}
\end{table}

\subsection{Estructura del Proyecto}

\begin{table}[H]
\centering
\scriptsize
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Directorio/Archivo} & \textbf{Descripci\'on} \\
\midrule
\textbf{config/} & \textbf{Configuraci\'on centralizada} \\
\hspace{0.3cm}settings.py & Settings con Pydantic (rutas, scraping, modelo) \\
\hspace{0.3cm}selectors.py & Selectores CSS organizados por entidad \\
\midrule
\textbf{src/scraper/} & \textbf{M\'odulo de web scraping} \\
\hspace{0.3cm}base.py & BaseScraper con Playwright, rate limiter, cache \\
\hspace{0.3cm}discovery.py & Descubrimiento de URLs de usuarios y submolts \\
\hspace{0.3cm}parsers.py & Funciones de parsing HTML con BeautifulSoup \\
\hspace{0.3cm}scrapers.py & MoltbookScraper: orquestador principal \\
\midrule
\textbf{src/database/} & \textbf{Capa de persistencia} \\
\hspace{0.3cm}models.py & Dataclasses (User, Post, Comment, SubMolt) \\
\hspace{0.3cm}connection.py & Context manager para conexiones SQLite \\
\hspace{0.3cm}operations.py & Operaciones CRUD con upsert gen\'erico \\
\midrule
\textbf{src/processing/} & \textbf{Procesamiento de datos} \\
\hspace{0.3cm}silver.py & Capa Silver: limpieza con Polars Lazy \\
\hspace{0.3cm}gold.py & Capa Gold: ingenier\'ia de features \\
\hspace{0.3cm}spark\_analysis.py & EDA y evaluaci\'on con PySpark \\
\midrule
\textbf{src/models/} & \textbf{Machine Learning} \\
\hspace{0.3cm}trainer.py & H2OTrainer con AutoML \\
\hspace{0.3cm}optimizer.py & Grid Search para optimizaci\'on GBM \\
\midrule
\textbf{app/} & \textbf{Interfaz CLI} \\
\hspace{0.3cm}\_\_main\_\_.py & Comandos Click: scrape, build, train, status \\
\midrule
\textbf{tests/} & \textbf{Pruebas unitarias} \\
\hspace{0.3cm}test\_parsers.py, test\_database.py & Tests con pytest y fixtures HTML \\
\midrule
\textbf{data/} & \textbf{Salidas del pipeline} \\
\hspace{0.3cm}moltbook.db & Base de datos SQLite \\
\hspace{0.3cm}raw/, silver/, gold/, models/ & Capas de datos y artefactos del modelo \\
\midrule
\textbf{Archivos ra\'iz} & \\
\hspace{0.3cm}schema.sql & DDL de la base de datos \\
\hspace{0.3cm}pyproject.toml & Metadatos y dependencias del proyecto \\
\hspace{0.3cm}notebooks/pipeline.ipynb & Notebook con ejecuci\'on completa \\
\bottomrule
\end{tabular}
\caption{Estructura del proyecto.}
\label{tab:estructura}
\end{table}

% ============================================================
\clearpage
\section{Configuraci\'on del Proyecto}

\subsection{Gesti\'on de Configuraci\'on con Pydantic}

La configuraci\'on del proyecto est\'a centralizada en la clase \texttt{Settings} utilizando \texttt{pydantic-settings}, que permite validar tipos autom\'aticamente y sobreescribir valores mediante variables de entorno con el prefijo \texttt{MOLTBOOK\_}.

\begin{lstlisting}[language=Python, caption={Clase Settings -- configuraci\'on centralizada}]
class Settings(BaseSettings):
    model_config = SettingsConfigDict(
        env_prefix="MOLTBOOK_",
        env_file=".env",
        env_file_encoding="utf-8",
        extra="ignore",
    )

    # Base paths
    project_root: Path = Field(
        default_factory=lambda: Path(__file__).parent.parent,
    )

    # Database settings
    db_type: Literal["sqlite", "postgres"] = Field(default="sqlite")
    db_path: str = Field(default="data/moltbook.db")

    # Scraping settings
    base_url: str = Field(default="https://www.moltbook.com")
    rate_limit_seconds: float = Field(default=1.0, ge=0.5)
    max_retries: int = Field(default=3, ge=1)
    request_timeout: int = Field(default=30)
    user_agent: str = Field(
        default="MoltbookScraper/1.0 (Academic Research Project)",
    )
    headless: bool = Field(default=True)

    # Model settings
    target_column: str = Field(default="karma")
    max_models: int = Field(default=10)
    max_runtime_secs: int = Field(default=300)

    def ensure_directories(self) -> None:
        for directory in [self.data_dir, self.raw_dir,
                          self.silver_dir, self.gold_dir,
                          self.models_dir]:
            directory.mkdir(parents=True, exist_ok=True)
\end{lstlisting}

Las propiedades \texttt{data\_dir}, \texttt{raw\_dir}, \texttt{silver\_dir}, \texttt{gold\_dir} y \texttt{models\_dir} se derivan autom\'aticamente a partir de \texttt{project\_root}, garantizando consistencia en las rutas de todo el pipeline.

\subsection{Selectores CSS Centralizados}

Para facilitar el mantenimiento del scraper ante cambios en la estructura HTML de moltbook.com, todos los selectores CSS est\'an centralizados en dataclasses inmutables organizadas por entidad.

\begin{lstlisting}[language=Python, caption={Selectores CSS organizados por entidad}]
@dataclass(frozen=True)
class UserProfileSelectors:
    name: str = "h1, h2.text-2xl"
    karma: str = "span:has-text('karma'), div:has-text('karma')"
    description: str = "p.text-\\[\\#818384\\], div.text-gray-400"
    human_owner: str = "a[href*='x.com'], a[href*='twitter.com']"
    followers_count: str = "span:has-text('followers')"
    following_count: str = "span:has-text('following')"

@dataclass(frozen=True)
class Selectors:
    user_list: UserListSelectors = UserListSelectors()
    user_profile: UserProfileSelectors = UserProfileSelectors()
    post: PostSelectors = PostSelectors()
    comment: CommentSelectors = CommentSelectors()
    submolt_list: SubMoltListSelectors = SubMoltListSelectors()
    submolt_page: SubMoltPageSelectors = SubMoltPageSelectors()
    navigation: NavigationSelectors = NavigationSelectors()

selectors = Selectors()
\end{lstlisting}

Este dise\~no permite modificar los selectores en un solo lugar cuando la estructura del sitio web cambia, sin necesidad de buscar selectores dispersos por el c\'odigo.

\subsection{Dependencias del Proyecto}

\begin{table}[H]
\centering
\begin{tabular}{@{}llp{6.5cm}@{}}
\toprule
\textbf{Librer\'ia} & \textbf{Versi\'on} & \textbf{Prop\'osito} \\
\midrule
playwright & $\geq$1.40.0 & Automatizaci\'on de navegador Chromium \\
beautifulsoup4 & $\geq$4.12.0 & Parsing HTML \\
polars & $\geq$1.0.0 & Procesamiento de datos con lazy evaluation \\
pyspark & $\geq$3.5.0 & An\'alisis distribuido y evaluaci\'on \\
h2o & $\geq$3.44.0 & AutoML para regresi\'on \\
pyarrow & $\geq$14.0.0 & Formato Parquet columnar \\
click & $\geq$8.1.0 & Framework CLI \\
tenacity & $\geq$8.2.0 & Reintentos con backoff exponencial \\
pydantic-settings & $\geq$2.1.0 & Configuraci\'on validada \\
matplotlib / seaborn & $\geq$3.8 / $\geq$0.13 & Visualizaci\'on \\
\bottomrule
\end{tabular}
\caption{Dependencias principales del proyecto.}
\end{table}

% ============================================================
\clearpage
\section{Web Scraping}

\subsection{Dise\~no del M\'odulo de Scraping}

El m\'odulo de scraping sigue una arquitectura por capas:

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        node distance=0.6cm and 1.5cm,
        component/.style={
            rectangle, rounded corners=3pt,
            minimum width=3.2cm, minimum height=1cm,
            text centered, font=\small,
            draw=accentblue!60, fill=lightblue,
        },
        arrow/.style={-{Stealth[length=2.5mm]}, thick, color=codegray},
    ]
        \node[component] (base) {\textbf{BaseScraper}\\{\tiny Playwright + Cache}};
        \node[component, right=of base] (discovery) {\textbf{URLDiscovery}\\{\tiny Descubrimiento}};
        \node[component, below=of base] (parsers) {\textbf{parsers.py}\\{\tiny Parsing HTML}};
        \node[component, below=of discovery] (orchestrator) {\textbf{MoltbookScraper}\\{\tiny Orquestador}};

        \draw[arrow] (orchestrator) -- (base);
        \draw[arrow] (orchestrator) -- (discovery);
        \draw[arrow] (orchestrator) -- (parsers);
        \draw[arrow] (discovery) -- (base);
    \end{tikzpicture}
    \caption{Arquitectura del m\'odulo de scraping.}
\end{figure}

\texttt{BaseScraper} provee la infraestructura de navegador, \texttt{URLDiscovery} se encarga del descubrimiento de URLs, las funciones de \texttt{parsers.py} extraen datos del HTML, y \texttt{MoltbookScraper} orquesta todo el flujo con persistencia incremental en la base de datos.

\subsection{Scraper Base}

La clase \texttt{BaseScraper} implementa la infraestructura de automatizaci\'on de navegador con Playwright. Incluye un \texttt{RateLimiter} para controlar la frecuencia de solicitudes y un decorador \texttt{@retry} de Tenacity para reintentos con backoff exponencial.

\begin{lstlisting}[language=Python, caption={RateLimiter para control de frecuencia}]
class RateLimiter:
    def __init__(self, min_interval: float):
        self.min_interval = min_interval
        self._last_request_time: float = 0

    def wait(self) -> None:
        elapsed = time.time() - self._last_request_time
        if elapsed < self.min_interval:
            sleep_time = self.min_interval - elapsed
            logger.debug("Rate limiting: sleeping %.2f seconds", sleep_time)
            time.sleep(sleep_time)
        self._last_request_time = time.time()
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={BaseScraper con Playwright y retry}]
class BaseScraper:
    def __init__(self, headless=True, rate_limit=None, cache_dir=None):
        self.headless = headless if headless is not None else settings.headless
        self.rate_limit = rate_limit or settings.rate_limit_seconds
        self.cache_dir = cache_dir or settings.raw_dir
        self._rate_limiter = RateLimiter(self.rate_limit)

    def start(self) -> None:
        self._playwright = sync_playwright().start()
        self._browser = self._playwright.chromium.launch(headless=self.headless)
        self._page = self._browser.new_page(user_agent=settings.user_agent)
        self._page.set_default_timeout(settings.request_timeout * 1000)

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        retry=retry_if_exception_type((TimeoutError, Exception)),
    )
    def fetch_page(self, url, wait_selector=None, wait_time=2000):
        self._rate_limiter.wait()
        self.page.goto(url)
        if wait_selector:
            try:
                self.page.wait_for_selector(wait_selector, timeout=10000)
            except Exception as e:
                logger.warning("Selector wait failed: %s", e)
        self.page.wait_for_timeout(wait_time)
        return self.page.content()
\end{lstlisting}

El m\'etodo \texttt{fetch\_with\_cache} combina la obtenci\'on de p\'aginas con un sistema de cach\'e en disco que evita solicitudes redundantes durante scraping incremental:

\begin{lstlisting}[language=Python, caption={Fetch con sistema de cache}]
    def fetch_with_cache(self, url, force_refresh=False, wait_selector=None):
        filename = self.get_cache_filename(url)
        if not force_refresh:
            cached = self.load_cached_html(filename)
            if cached:
                return cached
        html = self.fetch_page(url, wait_selector=wait_selector)
        self.save_html(html, filename)
        return html
\end{lstlisting}

\subsection{Descubrimiento de URLs}

La clase \texttt{URLDiscovery} descubre autom\'aticamente las URLs de perfiles de usuario y comunidades a partir de las p\'aginas de listado de moltbook.com.

\begin{lstlisting}[language=Python, caption={Descubrimiento de URLs de usuarios}]
class URLDiscovery:
    def discover_users(self, max_users=100, known_users=None):
        known_users = known_users or set()
        users_url = f"{self.base_url}/u"

        html = self.scraper.fetch_page(users_url, wait_selector="a[href^='/u/']")
        # Cambiar a vista por Karma para obtener los usuarios mas relevantes
        self.scraper.page.click("button:has-text('Karma')")
        time.sleep(2)

        self.scraper.scroll_to_load_all(max_scrolls=5)
        html = self.scraper.page.content()

        users = parse_users_list(html)
        urls = []
        for user in users:
            if len(urls) >= max_users:
                break
            username = user.get("name", "")
            if username and username not in known_users:
                urls.append(f"{self.base_url}/u/{username}")
        return urls
\end{lstlisting}

La estrategia de descubrimiento incluye: cambiar la vista a ``Karma'' para priorizar usuarios con mayor puntuaci\'on, y hacer scroll para cargar contenido din\'amico. El par\'ametro \texttt{known\_users} permite scraping incremental, omitiendo usuarios ya conocidos.

\subsection{Parsers HTML}

Las funciones de parsing siguen una estrategia defensiva: retornan valores por defecto (\texttt{None}, 0, o listas vac\'ias) ante cualquier fallo de extracci\'on, garantizando que un elemento faltante no detenga el pipeline completo.

\begin{lstlisting}[language=Python, caption={Parser de n\'umeros con sufijos K/M}]
def parse_number(text: str) -> int:
    if not text:
        return 0
    text = text.strip().lower()
    for word in ["karma", "followers", "following", "points", "members"]:
        text = text.replace(word, "").strip()

    multiplier = 1
    if text.endswith("k"):
        multiplier = 1000
        text = text[:-1]
    elif text.endswith("m"):
        multiplier = 1000000
        text = text[:-1]
    try:
        return int(float(text) * multiplier)
    except (ValueError, TypeError):
        return 0
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Parser de perfil de usuario}]
def parse_user_profile(html: str, username: str) -> Dict[str, Any]:
    soup = BeautifulSoup(html, "lxml")
    result = {
        "name": username, "karma": 0, "description": None,
        "human_owner": None, "joined": None, "followers": 0, "following": 0,
    }

    # Extraer karma con expresiones regulares
    karma_patterns = [
        re.compile(r"(\d+(?:\.\d+)?[KkMm]?)\s*karma", re.I),
        re.compile(r"karma[:\s]*(\d+(?:\.\d+)?[KkMm]?)", re.I),
    ]
    body_text = soup.get_text()
    for pattern in karma_patterns:
        match = pattern.search(body_text)
        if match:
            result["karma"] = parse_number(match.group(1))
            break

    # Extraer followers/following
    followers_match = re.search(
        r"(\d+(?:\.\d+)?[KkMm]?)\s*followers?", body_text, re.I)
    if followers_match:
        result["followers"] = parse_number(followers_match.group(1))
    following_match = re.search(
        r"(\d+(?:\.\d+)?[KkMm]?)\s*following", body_text, re.I)
    if following_match:
        result["following"] = parse_number(following_match.group(1))

    return result
\end{lstlisting}

Para el manejo de fechas relativas (``9d ago'', ``2h ago''), se implement\'o una funci\'on de conversi\'on:

\begin{lstlisting}[language=Python, caption={Conversi\'on de fechas relativas}]
def convert_relative_to_date(text: str) -> str:
    now = datetime.now()
    match = re.search(r'(\d+)([a-z]+)', text.lower())
    if not match:
        return now.strftime("%Y-%m-%d")

    value = int(match.group(1))
    unit = match.group(2)

    if 'd' in unit:
        date_obj = now - timedelta(days=value)
    elif 'h' in unit:
        date_obj = now - timedelta(hours=value)
    elif 'm' in unit:
        date_obj = now - timedelta(minutes=value)
    elif 'w' in unit:
        date_obj = now - timedelta(weeks=value)
    elif 'mo' in unit:
        date_obj = now - timedelta(days=value * 30)
    else:
        date_obj = now
    return date_obj.strftime("%Y-%m-%d")
\end{lstlisting}

\begin{table}[H]
\centering
\scriptsize
\begin{tabular}{@{}lllp{4.5cm}@{}}
\toprule
\textbf{Funci\'on} & \textbf{Entrada} & \textbf{Salida} & \textbf{Datos extra\'idos} \\
\midrule
parse\_users\_list & HTML de /u & List[Dict] & nombre, karma, URL \\
parse\_user\_profile & HTML de /u/\{user\} & Dict & karma, descripci\'on, seguidores \\
parse\_submolt\_list & HTML de /m & List[Dict] & nombre, descripci\'on \\
parse\_posts\_from\_page & HTML con posts & List[Dict] & t\'itulo, autor, rating, fecha \\
parse\_comments & HTML de post & List[Dict] & autor, contenido, rating, fecha \\
\bottomrule
\end{tabular}
\caption{Resumen de funciones de parsing.}
\end{table}

\subsection{Orquestaci\'on del Scraping}

La clase \texttt{MoltbookScraper} orquesta el flujo completo de scraping utilizando el patr\'on context manager para gestionar el ciclo de vida del navegador. Implementa persistencia incremental, guardando cada entidad en la base de datos inmediatamente despu\'es de ser extra\'ida.

\begin{lstlisting}[language=Python, caption={Orquestaci\'on del scraping completo}]
class MoltbookScraper:
    def __enter__(self) -> "MoltbookScraper":
        self._scraper = BaseScraper(headless=self.headless)
        self._scraper.start()
        self._discovery = URLDiscovery(self._scraper)
        return self

    def scrape_all(self, max_users=10, max_submolts=5,
                   max_posts=10, max_comments=100,
                   force_refresh=False) -> dict:
        users = self.scrape_users(max_users=max_users,
                                  force_refresh=force_refresh)
        submolts = self.scrape_submolts(max_submolts=max_submolts,
                                        force_refresh=force_refresh,
                                        max_posts=max_posts,
                                        max_comments=max_comments)
        post_count = self.db_ops.count(Post)
        comment_count = self.db_ops.count(Comment)
        return {
            "users": len(users),
            "submolts": len(submolts),
            "posts": post_count,
            "comments": comment_count,
        }
\end{lstlisting}

El flujo interno sigue esta secuencia: (1) descubrir URLs de usuarios y scrapear perfiles, (2) descubrir URLs de submolts y scrapear sus p\'aginas, (3) para cada submolt extraer sus posts, y (4) para cada post navegar a su p\'agina y extraer comentarios. Cada entidad se persiste con upsert inmediato en la base de datos.

\subsection{Scraping \'Etico}

El scraper implementa pr\'acticas \'eticas de recolecci\'on de datos:
\begin{itemize}
    \item \textbf{Rate limiting:} M\'inimo 1 segundo entre solicitudes (configurable).
    \item \textbf{User-Agent:} Identificaci\'on como ``MoltbookScraper/1.0 (Academic Research Project)''.
    \item \textbf{Cach\'e HTML:} Almacenamiento local de p\'aginas para evitar solicitudes redundantes.
    \item \textbf{Scraping incremental:} Se omiten usuarios y comunidades ya conocidos.
    \item \textbf{Reintentos controlados:} M\'aximo 3 intentos con backoff exponencial.
\end{itemize}

% ============================================================
\clearpage
\section{Base de Datos}

\subsection{Dise\~no del Esquema Relacional}

El esquema de la base de datos SQLite consta de 5 tablas con relaciones de foreign key, \'indices de rendimiento, y journal mode WAL para escritura concurrente.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        entity/.style={
            rectangle, rounded corners=2pt,
            minimum width=3cm, minimum height=0.8cm,
            text centered, font=\small\bfseries,
            draw=accentblue, fill=lightblue,
        },
        relation/.style={
            diamond, aspect=2, minimum width=1.5cm,
            text centered, font=\tiny,
            draw=accentorange, fill=lightorange,
        },
        arrow/.style={-{Stealth[length=2mm]}, thick, color=codegray},
    ]
        \node[entity] (users) {users};
        \node[entity, right=3cm of users] (posts) {posts};
        \node[entity, below=2cm of posts] (comments) {comments};
        \node[entity, below=2cm of users] (submolt) {sub\_molt};
        \node[entity, below=1cm of users, xshift=1.5cm] (usub) {\tiny user\_submolt};

        \draw[arrow] (posts) -- node[above, font=\tiny] {id\_user} (users);
        \draw[arrow] (comments) -- node[right, font=\tiny] {id\_post} (posts);
        \draw[arrow] (comments.west) -- node[below, font=\tiny] {id\_user} (users.east|-comments.west);
        \draw[arrow] (posts) -- node[right, font=\tiny, pos=0.3] {id\_submolt} (submolt.east|-posts.south);
        \draw[arrow] (usub) -- node[left, font=\tiny] {} (users);
        \draw[arrow] (usub) -- node[right, font=\tiny] {} (submolt);
    \end{tikzpicture}
    \caption{Diagrama entidad-relaci\'on de la base de datos.}
    \label{fig:er}
\end{figure}

\begin{lstlisting}[language=SQL, caption={Esquema DDL de la base de datos}]
PRAGMA foreign_keys = ON;

CREATE TABLE IF NOT EXISTS users (
    id_user TEXT PRIMARY KEY,
    name TEXT NOT NULL,
    karma INTEGER DEFAULT 0,
    description TEXT,
    human_owner TEXT,
    joined TEXT,
    followers INTEGER DEFAULT 0,
    following INTEGER DEFAULT 0,
    scraped_at TEXT NOT NULL
);

CREATE TABLE IF NOT EXISTS sub_molt (
    id_submolt TEXT PRIMARY KEY,
    name TEXT NOT NULL UNIQUE,
    description TEXT,
    scraped_at TEXT NOT NULL
);

CREATE TABLE IF NOT EXISTS posts (
    id_post TEXT PRIMARY KEY,
    id_user TEXT NOT NULL,
    id_submolt TEXT,
    title TEXT,
    description TEXT,
    rating INTEGER DEFAULT 0,
    date TEXT,
    scraped_at TEXT NOT NULL,
    FOREIGN KEY (id_user) REFERENCES users(id_user),
    FOREIGN KEY (id_submolt) REFERENCES sub_molt(id_submolt)
);

CREATE TABLE IF NOT EXISTS comments (
    id_comment TEXT PRIMARY KEY,
    id_user TEXT NOT NULL,
    id_post TEXT NOT NULL,
    description TEXT,
    date TEXT,
    rating INTEGER DEFAULT 0,
    scraped_at TEXT NOT NULL,
    FOREIGN KEY (id_user) REFERENCES users(id_user),
    FOREIGN KEY (id_post) REFERENCES posts(id_post)
);

CREATE TABLE IF NOT EXISTS user_submolt (
    id_user TEXT NOT NULL,
    id_submolt TEXT NOT NULL,
    PRIMARY KEY (id_user, id_submolt),
    FOREIGN KEY (id_user) REFERENCES users(id_user),
    FOREIGN KEY (id_submolt) REFERENCES sub_molt(id_submolt)
);

-- Indices para rendimiento de consultas
CREATE INDEX IF NOT EXISTS idx_posts_user ON posts(id_user);
CREATE INDEX IF NOT EXISTS idx_posts_submolt ON posts(id_submolt);
CREATE INDEX IF NOT EXISTS idx_comments_user ON comments(id_user);
CREATE INDEX IF NOT EXISTS idx_comments_post ON comments(id_post);
CREATE INDEX IF NOT EXISTS idx_users_karma ON users(karma);
CREATE INDEX IF NOT EXISTS idx_users_name ON users(name);
\end{lstlisting}

\subsection{Modelos de Datos}

Los modelos de datos se implementan como dataclasses de Python con factory methods \texttt{from\_scraped\_data()} para crear instancias a partir de datos extra\'idos, y m\'etodos \texttt{to\_dict()} para serializaci\'on hacia la base de datos.

Los identificadores se generan de forma determinista utilizando SHA-256:

\begin{lstlisting}[language=Python, caption={Generaci\'on determinista de IDs con SHA-256}]
def generate_id(prefix: str, *args: str) -> str:
    content = "|".join(str(arg) for arg in args if arg)
    hash_value = hashlib.sha256(content.encode()).hexdigest()[:12]
    return f"{prefix}_{hash_value}"
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Modelo User con factory method}]
@dataclass
class User:
    id_user: str
    name: str
    karma: int = 0
    description: Optional[str] = None
    human_owner: Optional[str] = None
    joined: Optional[str] = None
    followers: int = 0
    following: int = 0
    scraped_at: str = field(default_factory=get_timestamp)

    @classmethod
    def from_scraped_data(cls, name, karma=0, description=None,
                          human_owner=None, joined=None,
                          followers=0, following=0) -> "User":
        id_user = generate_id("user", name.lower())
        return cls(id_user=id_user, name=name, karma=karma,
                   description=description, human_owner=human_owner,
                   joined=joined, followers=followers, following=following)

    def to_dict(self) -> dict:
        return {
            "id_user": self.id_user, "name": self.name,
            "karma": self.karma, "description": self.description,
            "human_owner": self.human_owner, "joined": self.joined,
            "followers": self.followers, "following": self.following,
            "scraped_at": self.scraped_at,
        }
\end{lstlisting}

\begin{table}[H]
\centering
\scriptsize
\begin{tabular}{@{}lllp{5cm}@{}}
\toprule
\textbf{Entidad} & \textbf{Tabla} & \textbf{Clave Primaria} & \textbf{Campos principales} \\
\midrule
User & users & id\_user (SHA-256) & name, karma, followers, following, description \\
Post & posts & id\_post & title, description, rating, date \\
Comment & comments & id\_comment & description, rating, date \\
SubMolt & sub\_molt & id\_submolt & name, description \\
UserSubMolt & user\_submolt & (id\_user, id\_submolt) & Relaci\'on M:N \\
\bottomrule
\end{tabular}
\caption{Resumen de entidades del modelo de datos.}
\end{table}

\subsection{Conexi\'on y Gesti\'on de la Base de Datos}

La gesti\'on de conexiones utiliza un context manager que garantiza commit autom\'atico en caso de \'exito y rollback en caso de error:

\begin{lstlisting}[language=Python, caption={Context manager para conexiones SQLite}]
@contextmanager
def get_connection(db_path=None):
    conn = get_sqlite_connection(db_path)
    try:
        yield conn
        conn.commit()
    except Exception as e:
        conn.rollback()
        logger.error("Database error: %s", e)
        raise
    finally:
        conn.close()
\end{lstlisting}

La funci\'on \texttt{get\_sqlite\_connection} configura la conexi\'on con \texttt{PRAGMA foreign\_keys = ON} para habilitar integridad referencial y \texttt{PRAGMA journal\_mode = WAL} para mejorar el rendimiento de escritura.

\subsection{Operaciones CRUD con Upsert Gen\'erico}

La clase \texttt{DatabaseOperations} implementa un mecanismo de upsert gen\'erico que funciona con cualquier tipo de entidad, generando din\'amicamente las sentencias SQL de \texttt{INSERT ... ON CONFLICT DO UPDATE}:

\begin{lstlisting}[language=Python, caption={Upsert gen\'erico para cualquier entidad}]
class DatabaseOperations:
    TABLE_MAPPING = {
        User: "users", Post: "posts", Comment: "comments",
        SubMolt: "sub_molt", UserSubMolt: "user_submolt",
    }
    PK_MAPPING = {
        User: "id_user", Post: "id_post", Comment: "id_comment",
        SubMolt: "id_submolt", UserSubMolt: ("id_user", "id_submolt"),
    }

    def upsert(self, entity: T) -> bool:
        entity_type = type(entity)
        table = self.TABLE_MAPPING[entity_type]
        data = entity.to_dict()
        columns = list(data.keys())
        placeholders = ", ".join("?" for _ in columns)
        column_names = ", ".join(columns)

        pk = self.PK_MAPPING[entity_type]
        update_cols = [c for c in columns
                       if c not in (pk if isinstance(pk, tuple) else [pk])]
        update_clause = ", ".join(
            f"{col} = excluded.{col}" for col in update_cols)

        sql = f"""
            INSERT INTO {table} ({column_names})
            VALUES ({placeholders})
            ON CONFLICT DO UPDATE SET {update_clause}
        """
        with get_connection(self.db_path) as conn:
            conn.execute(sql, list(data.values()))
            return True
\end{lstlisting}

Este dise\~no evita duplicados y asegura que los registros siempre contengan la informaci\'on m\'as reciente. El m\'etodo \texttt{upsert\_many} extiende esta funcionalidad para operaciones por lotes.

% ============================================================
\clearpage
\section{Procesamiento de Datos -- Arquitectura Medallion}

\subsection{Visi\'on General de la Arquitectura}

El procesamiento de datos sigue la arquitectura Medallion, un patr\'on de dise\~no que organiza los datos en capas progresivamente refinadas:

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        layer/.style={
            rectangle, rounded corners=6pt,
            minimum width=4cm, minimum height=1.8cm,
            text centered, font=\small,
            thick,
        },
        arrow/.style={-{Stealth[length=3mm]}, very thick},
    ]
        \node[layer, draw=accentblue, fill=accentblue!10] (bronze) {
            \textbf{Bronze}\\[2pt]
            {\scriptsize SQLite -- Datos crudos}\\
            {\scriptsize 4 tablas, FK, WAL}
        };
        \node[layer, draw=accentgreen!80, fill=accentgreen!10, right=1.5cm of bronze] (silver) {
            \textbf{Silver}\\[2pt]
            {\scriptsize Polars Lazy -- Limpieza}\\
            {\scriptsize 4 archivos Parquet}
        };
        \node[layer, draw=accentorange, fill=accentorange!10, right=1.5cm of silver] (gold) {
            \textbf{Gold}\\[2pt]
            {\scriptsize Feature Engineering}\\
            {\scriptsize 18 features + target}
        };

        \draw[arrow, color=accentblue] (bronze) -- (silver);
        \draw[arrow, color=accentgreen!80] (silver) -- (gold);
    \end{tikzpicture}
    \caption{Arquitectura Medallion: Bronze $\rightarrow$ Silver $\rightarrow$ Gold.}
    \label{fig:medallion}
\end{figure}

\begin{itemize}
    \item \textbf{Bronze (SQLite):} Datos crudos tal como se obtienen del scraping.
    \item \textbf{Silver (Parquet):} Datos limpios, con tipos correctos, sin duplicados y valores nulos tratados.
    \item \textbf{Gold (Parquet):} Features ingenierizados listos para el modelado.
\end{itemize}

Se utiliza \textbf{Polars con evaluaci\'on lazy} en ambas capas, lo que permite construir grafos de ejecuci\'on optimizados que minimizan el uso de memoria y maximizan el rendimiento al posponer la ejecuci\'on hasta el momento de la materializaci\'on con \texttt{.collect()}.

\subsection{Capa Silver -- Limpieza y Validaci\'on}

La capa Silver carga los datos crudos desde SQLite, los limpia y los almacena en formato Parquet columnar.

\begin{lstlisting}[language=Python, caption={Carga de tabla SQLite a Polars LazyFrame}]
def load_table_to_lazy(table_name: str) -> pl.LazyFrame:
    db_path = settings.project_root / settings.db_path
    query = f"SELECT * FROM {table_name}"
    df = pl.read_database_uri(
        query=query,
        uri=f"sqlite:///{db_path}",
    )
    logger.info("Loaded %d rows from %s", len(df), table_name)
    return df.lazy()
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Limpieza de usuarios con Polars Lazy}]
def clean_users(lf: pl.LazyFrame) -> pl.LazyFrame:
    return (
        lf
        .with_columns([
            pl.col("karma").cast(pl.Int64).fill_null(0),
            pl.col("followers").cast(pl.Int64).fill_null(0),
            pl.col("following").cast(pl.Int64).fill_null(0),
            pl.col("name").str.strip_chars(),
            pl.col("description").str.strip_chars().fill_null(""),
            pl.col("human_owner").str.strip_chars(),
            pl.col("joined").str.strip_chars(),
        ])
        .unique(subset=["id_user"], keep="last")
        .filter(pl.col("name").str.len_chars() > 0)
    )
\end{lstlisting}

\begin{table}[H]
\centering
\scriptsize
\begin{tabular}{@{}lp{8.5cm}@{}}
\toprule
\textbf{Entidad} & \textbf{Operaciones de limpieza} \\
\midrule
Users & Cast karma/followers/following a Int64, fill\_null(0), strip strings, deduplicar por id\_user, filtrar nombres vac\'ios \\
Posts & Cast rating a Int64, fill\_null(0), strip title/description, deduplicar por id\_post \\
Comments & Cast rating a Int64, fill\_null(0), strip description, deduplicar por id\_comment \\
SubMolts & Strip name/description, fill\_null(``''), deduplicar por id\_submolt, filtrar nombres vac\'ios \\
\bottomrule
\end{tabular}
\caption{Operaciones de limpieza por entidad en la capa Silver.}
\end{table}

La funci\'on \texttt{build\_silver\_layer} orquesta el procesamiento de las cuatro tablas, generando archivos Parquet independientes en el directorio \texttt{data/silver/}.

\subsection{Capa Gold -- Ingenier\'ia de Features}

La capa Gold toma los datos limpios de la capa Silver y construye un dataset de features listo para el modelado. Se agregan estad\'isticas de posts y comentarios por usuario, y se derivan features como ratios y flags binarios.

\begin{lstlisting}[language=Python, caption={Ingenier\'ia de features para predicci\'on de karma}]
def engineer_user_features(users_lf, posts_lf, comments_lf):
    # Agregar estadisticas de posts por usuario
    post_stats = (
        posts_lf
        .group_by("id_user")
        .agg([
            pl.count().alias("post_count"),
            pl.col("rating").sum().alias("total_post_rating"),
            pl.col("rating").mean().alias("avg_post_rating"),
            pl.col("rating").max().alias("max_post_rating"),
            pl.col("title").str.len_chars().mean().alias("avg_title_length"),
            pl.col("description").str.len_chars().mean()
                .alias("avg_post_desc_length"),
        ])
    )
    # Agregar estadisticas de comentarios por usuario
    comment_stats = (
        comments_lf
        .group_by("id_user")
        .agg([
            pl.count().alias("comment_count"),
            pl.col("rating").sum().alias("total_comment_rating"),
            pl.col("rating").mean().alias("avg_comment_rating"),
            pl.col("description").str.len_chars().mean()
                .alias("avg_comment_length"),
        ])
    )
    # Unir features y crear features derivados
    features = (
        users_lf
        .join(post_stats, on="id_user", how="left")
        .join(comment_stats, on="id_user", how="left")
        .with_columns([  # Fill nulls
            pl.col("post_count").fill_null(0),
            pl.col("comment_count").fill_null(0),
            # ... (fill_null para todas las columnas)
        ])
        .with_columns([  # Features derivados
            (pl.col("followers") / (pl.col("following") + 1))
                .alias("follower_ratio"),
            (pl.col("post_count") + pl.col("comment_count"))
                .alias("total_activity"),
            (pl.col("total_post_rating") + pl.col("total_comment_rating"))
                .alias("total_rating"),
            (pl.col("description").str.len_chars() > 0)
                .cast(pl.Int32).alias("has_description"),
            pl.col("human_owner").is_not_null()
                .cast(pl.Int32).alias("has_human_owner"),
            pl.col("description").str.len_chars()
                .alias("description_length"),
        ])
    )
    return features
\end{lstlisting}

\begin{table}[H]
\centering
\scriptsize
\begin{tabular}{@{}clll@{}}
\toprule
\textbf{\#} & \textbf{Feature} & \textbf{Tipo} & \textbf{Descripci\'on} \\
\midrule
1 & followers & Int64 & N\'umero de seguidores \\
2 & following & Int64 & N\'umero de seguidos \\
3 & follower\_ratio & Float64 & followers / (following + 1) \\
4 & has\_description & Int32 & Flag binario de descripci\'on \\
5 & has\_human\_owner & Int32 & Flag binario de propietario humano \\
6 & description\_length & UInt32 & Longitud de la descripci\'on \\
7 & post\_count & UInt32 & N\'umero de publicaciones \\
8 & total\_post\_rating & Int64 & Suma de puntuaciones de posts \\
9 & avg\_post\_rating & Float64 & Promedio de puntuaciones de posts \\
10 & max\_post\_rating & Int64 & Puntuaci\'on m\'axima de un post \\
11 & avg\_title\_length & Float64 & Longitud promedio de t\'itulos \\
12 & avg\_post\_desc\_length & Float64 & Longitud promedio de descripciones \\
13 & comment\_count & UInt32 & N\'umero de comentarios \\
14 & total\_comment\_rating & Int64 & Suma de puntuaciones de comentarios \\
15 & avg\_comment\_rating & Float64 & Promedio de puntuaciones de comentarios \\
16 & avg\_comment\_length & Float64 & Longitud promedio de comentarios \\
17 & total\_activity & UInt32 & post\_count + comment\_count \\
18 & total\_rating & Int64 & total\_post\_rating + total\_comment\_rating \\
\bottomrule
\end{tabular}
\caption{Features ingenierizados para predicci\'on de karma (18 variables).}
\label{tab:features}
\end{table}

% ============================================================
\clearpage
\section{An\'alisis Exploratorio con PySpark}

\subsection{Configuraci\'on de SparkSession}

Se utiliza PySpark en modo local para complementar el procesamiento de Polars con capacidades de an\'alisis distribuido y evaluaci\'on de modelos.

\begin{lstlisting}[language=Python, caption={Configuraci\'on de SparkSession local}]
def create_spark_session(app_name: str = "MoltbookKarma"):
    from pyspark.sql import SparkSession
    spark = (
        SparkSession.builder
        .appName(app_name)
        .master("local[*]")
        .config("spark.driver.memory", "2g")
        .config("spark.sql.shuffle.partitions", "4")
        .config("spark.ui.showConsoleProgress", "false")
        .getOrCreate()
    )
    spark.sparkContext.setLogLevel("WARN")
    return spark
\end{lstlisting}

\subsection{EDA -- An\'alisis Exploratorio de Datos}

La funci\'on \texttt{spark\_eda} ejecuta un an\'alisis completo que incluye: conteo de registros, estad\'isticas descriptivas, an\'alisis de nulos, distribuci\'on del karma por bins, top de usuarios, y correlaciones.

\begin{lstlisting}[language=Python, caption={Distribuci\'on del karma por bins con PySpark}]
karma_bins = (
    users_df
    .withColumn("karma_bin",
        F.when(F.col("karma") == 0, "0")
        .when(F.col("karma") <= 10, "1-10")
        .when(F.col("karma") <= 100, "11-100")
        .when(F.col("karma") <= 1000, "101-1K")
        .when(F.col("karma") <= 10000, "1K-10K")
        .otherwise(">10K"))
    .groupBy("karma_bin")
    .agg(F.count("*").alias("count"))
    .orderBy("count", ascending=False)
)
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={C\'alculo de correlaciones karma vs. m\'etricas}]
numeric_cols = ["karma", "followers", "following"]
correlations = {}
for col_name in numeric_cols:
    if col_name != "karma":
        corr_val = users_df.stat.corr("karma", col_name)
        correlations[f"karma_vs_{col_name}"] = round(corr_val, 4)
\end{lstlisting}

\begin{keyinsight}[Hallazgos del EDA]
El an\'alisis exploratorio revel\'o una distribuci\'on de karma altamente asim\'etrica: la mayor\'ia de usuarios tienen karma = 0 (mediana = 0), mientras que el m\'aximo es 500,002 y la media es 6,553. Solo 41 de 981 usuarios tienen descripci\'on de perfil, y el promedio de seguidores es 0.46.
\end{keyinsight}

\subsection{Evaluaci\'on de Predicciones con PySpark}

La funci\'on \texttt{spark\_evaluate\_predictions} utiliza el \texttt{RegressionEvaluator} de PySpark ML para evaluar las predicciones del modelo, calculando MAE, RMSE, R$^2$ y MSE. Tambi\'en realiza an\'alisis de residuos y evaluaci\'on por cuartiles.

\begin{lstlisting}[language=Python, caption={Evaluaci\'on con RegressionEvaluator de PySpark}]
def spark_evaluate_predictions(predictions_path=None, gold_dir=None):
    from pyspark.ml.evaluation import RegressionEvaluator

    pred_df = spark.read.parquet(str(predictions_path))
    eval_df = (
        pred_df
        .withColumnRenamed("karma", "label")
        .withColumnRenamed("karma_predicted", "prediction")
    )

    evaluators = {
        "mae": RegressionEvaluator(metricName="mae"),
        "rmse": RegressionEvaluator(metricName="rmse"),
        "r2": RegressionEvaluator(metricName="r2"),
        "mse": RegressionEvaluator(metricName="mse"),
    }

    for metric_name, evaluator in evaluators.items():
        value = evaluator.evaluate(eval_df)
        results[metric_name] = round(value, 4)

    # Analisis de residuos
    residuals_df = (
        eval_df
        .withColumn("residual", F.col("label") - F.col("prediction"))
        .withColumn("abs_residual", F.abs(F.col("residual")))
    )
    return results
\end{lstlisting}

El an\'alisis por cuartiles permite evaluar c\'omo se comporta el modelo en diferentes rangos de karma, identificando si las predicciones son m\'as precisas para usuarios con karma bajo, medio o alto.

% ============================================================
\clearpage
\section{Pipeline de Machine Learning}

\subsection{Dise\~no del M\'odulo de Entrenamiento}

El m\'odulo de Machine Learning utiliza \textbf{H2O AutoML}, una plataforma de aprendizaje autom\'atico que explora autom\'aticamente m\'ultiples algoritmos (GBM, XGBoost, Random Forest, GLM, entre otros) y selecciona el mejor modelo basado en la m\'etrica de RMSE.

\subsection{Features Utilizados para el Modelo}

De las 18 features ingenierizadas en la capa Gold, H2O AutoML identific\'o que \texttt{avg\_comment\_rating} y \texttt{total\_comment\_rating} ten\'ian varianza cero y fueron descartadas autom\'aticamente. Las 16 features restantes se utilizan como variables predictoras:

\begin{lstlisting}[language=Python, caption={Columnas de features para el modelo}]
FEATURE_COLUMNS = [
    "followers", "following", "follower_ratio",
    "has_description", "has_human_owner", "description_length",
    "post_count", "total_post_rating", "avg_post_rating",
    "max_post_rating", "avg_title_length", "avg_post_desc_length",
    "comment_count", "total_comment_rating", "avg_comment_rating",
    "avg_comment_length", "total_activity", "total_rating",
]
\end{lstlisting}

\subsection{Clase H2OTrainer}

La clase \texttt{H2OTrainer} encapsula todo el flujo de entrenamiento: inicializaci\'on del cluster H2O, conversi\'on de datos, divisi\'on train/test, ejecuci\'on de AutoML, y persistencia del modelo.

\begin{lstlisting}[language=Python, caption={Inicializaci\'on y entrenamiento con H2O AutoML}]
class H2OTrainer:
    def __init__(self, max_models=10, max_runtime_secs=300, seed=42):
        self.max_models = max_models
        self.max_runtime_secs = max_runtime_secs
        self.seed = seed

    def _init_h2o(self) -> None:
        import h2o
        h2o.init(nthreads=-1, max_mem_size="4G")
        self._h2o = h2o

    def train(self, data, target="karma", features=None, test_size=0.2):
        self._init_h2o()
        features = features or FEATURE_COLUMNS
        available_features = [f for f in features if f in data.columns]

        # Convertir a H2O frame y dividir
        h2o_data = self._h2o.H2OFrame(data.to_pandas())
        train, test = h2o_data.split_frame(
            ratios=[1 - test_size], seed=self.seed)

        # Ejecutar AutoML
        from h2o.automl import H2OAutoML
        aml = H2OAutoML(
            max_models=self.max_models,
            max_runtime_secs=self.max_runtime_secs,
            seed=self.seed,
            sort_metric="RMSE",
            exclude_algos=["DeepLearning"],
        )
        aml.train(x=available_features, y=target,
                  training_frame=train, validation_frame=test)

        self._model = aml.leader
        perf = self._model.model_performance(test)
        return {
            "model_id": self._model.model_id,
            "mae": perf.mae(), "rmse": perf.rmse(), "r2": perf.r2(),
            "train_size": train.nrows, "test_size": test.nrows,
        }
\end{lstlisting}

Las decisiones de dise\~no clave son: excluir DeepLearning para acelerar el entrenamiento sin p\'erdida significativa de rendimiento, usar RMSE como m\'etrica de ordenamiento, y dividir 80/20 con semilla fija para reproducibilidad.

\begin{lstlisting}[language=Python, caption={Predicci\'on y persistencia del modelo}]
    def predict(self, data: pl.DataFrame) -> pl.DataFrame:
        h2o_data = self._h2o.H2OFrame(data.to_pandas())
        predictions = self._model.predict(h2o_data)
        pred_df = pl.from_pandas(predictions.as_data_frame())
        pred_df = pred_df.rename({"predict": "karma_predicted"})
        return data.with_columns([pred_df["karma_predicted"]])

    def save_model(self, output_dir=None) -> Path:
        output_dir = output_dir or settings.models_dir
        output_dir.mkdir(parents=True, exist_ok=True)
        model_path = self._h2o.save_model(
            model=self._model, path=str(output_dir), force=True)
        return Path(model_path)
\end{lstlisting}

\subsection{Optimizaci\'on con Grid Search}

Despu\'es de que AutoML identifica GBM como el algoritmo con mejor rendimiento, se ejecuta una optimizaci\'on espec\'ifica mediante Grid Search sobre los hiperpar\'ametros de GBM.

\begin{lstlisting}[language=Python, caption={Grid de hiperpar\'ametros para GBM}]
DEFAULT_GBM_GRID = {
    "max_depth": [3, 5, 7, 10],
    "learn_rate": [0.01, 0.05, 0.1],
    "ntrees": [50, 100, 200],
    "sample_rate": [0.7, 0.8, 1.0],
}
\end{lstlisting}

\begin{table}[H]
\centering
\begin{tabular}{@{}lp{7cm}@{}}
\toprule
\textbf{Hiperpar\'ametro} & \textbf{Valores explorados} \\
\midrule
max\_depth & 3, 5, 7, 10 \\
learn\_rate & 0.01, 0.05, 0.1 \\
ntrees & 50, 100, 200 \\
sample\_rate & 0.7, 0.8, 1.0 \\
\bottomrule
\end{tabular}
\caption{Espacio de b\'usqueda del Grid Search para GBM.}
\end{table}

El espacio total de combinaciones es $4 \times 3 \times 3 \times 3 = 108$, pero se utiliza una estrategia \texttt{RandomDiscrete} con un m\'aximo de 15 modelos y 300 segundos de tiempo l\'imite para explorar eficientemente el espacio.

\begin{lstlisting}[language=Python, caption={Optimizaci\'on GBM con Grid Search}]
def optimize_gbm(train_frame, valid_frame, target="karma",
                 features=None, hyper_params=None):
    from h2o.estimators import H2OGradientBoostingEstimator
    from h2o.grid.grid_search import H2OGridSearch

    hyper_params = hyper_params or DEFAULT_GBM_GRID

    gbm = H2OGradientBoostingEstimator(
        seed=42, stopping_metric="MAE",
        stopping_rounds=5, stopping_tolerance=0.001,
    )

    grid = H2OGridSearch(
        model=gbm,
        hyper_params=hyper_params,
        grid_id="gbm_optimization_grid",
        search_criteria={
            "strategy": "RandomDiscrete",
            "max_models": 15,
            "max_runtime_secs": 300,
            "seed": 42,
        },
    )

    grid.train(x=features, y=target,
               training_frame=train_frame,
               validation_frame=valid_frame)

    grid_table = grid.get_grid(sort_by="mae", decreasing=False)
    best_model = grid_table.models[0]
    best_perf = best_model.model_performance(valid_frame)
    return {
        "best_model": best_model,
        "best_params": {
            "max_depth": best_model.params["max_depth"]["actual"],
            "learn_rate": best_model.params["learn_rate"]["actual"],
            "ntrees": best_model.params["ntrees"]["actual"],
            "sample_rate": best_model.params["sample_rate"]["actual"],
        },
        "mae": best_perf.mae(), "rmse": best_perf.rmse(),
        "r2": best_perf.r2(),
    }
\end{lstlisting}

La funci\'on \texttt{compare\_models} genera una tabla comparativa entre el modelo base de AutoML y el modelo optimizado, calculando el porcentaje de mejora en cada m\'etrica.

\subsection{Funci\'on Principal de Entrenamiento}

La funci\'on \texttt{train\_model} orquesta el flujo completo: carga datos del Gold layer, entrena con AutoML, guarda el modelo y genera predicciones.

\begin{lstlisting}[language=Python, caption={Orquestaci\'on del entrenamiento}]
def train_model(gold_dir=None, output_dir=None,
                max_models=10, max_runtime_secs=300):
    data = get_modeling_data(gold_dir)

    trainer = H2OTrainer(
        max_models=max_models,
        max_runtime_secs=max_runtime_secs,
    )
    results = trainer.train(data, target="karma")

    # Guardar modelo y predicciones
    model_path = trainer.save_model(output_dir)
    predictions = trainer.predict(data)
    pred_path = output_dir / "predictions.parquet"
    predictions.write_parquet(pred_path)
    return results
\end{lstlisting}

% ============================================================
\clearpage
\section{Interfaz de L\'inea de Comandos (CLI)}

\subsection{Dise\~no de la CLI con Click}

El proyecto ofrece una CLI construida con Click que permite ejecutar cada etapa del pipeline de forma independiente o secuencial.

\begin{table}[H]
\centering
\begin{tabular}{@{}llp{6cm}@{}}
\toprule
\textbf{Comando} & \textbf{Opciones principales} & \textbf{Descripci\'on} \\
\midrule
scrape & --max-users, --max-posts, --force, --headless & Scraping de moltbook.com \\
build & (sin opciones) & Construcci\'on de capas Silver y Gold \\
train & --max-models, --max-time & Entrenamiento del modelo H2O \\
status & (sin opciones) & Estado actual del pipeline \\
\bottomrule
\end{tabular}
\caption{Referencia de comandos CLI.}
\end{table}

\begin{lstlisting}[language=Python, caption={Definici\'on de la CLI con Click}]
@click.group()
@click.option("-v", "--verbose", is_flag=True, help="Enable verbose logging")
def cli(verbose: bool) -> None:
    """Moltbook Karma Data Engineering Pipeline."""
    setup_logging(verbose)
    settings.ensure_directories()

@cli.command()
@click.option("--max-users", default=100)
@click.option("--max-posts", default=500)
@click.option("--force", is_flag=True)
@click.option("--headless/--no-headless", default=True)
def scrape(max_users, max_posts, max_comments, force, headless):
    if not check_database_exists():
        init_database()
    with MoltbookScraper(headless=headless) as scraper:
        results = scraper.scrape_all(
            max_users=max_users, max_posts=max_posts,
            max_comments=max_comments, force_refresh=force,
        )
    click.echo(f"Users: {results['users']}, Posts: {results['posts']}")
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Comando status -- estado del pipeline}]
@cli.command()
def status() -> None:
    click.echo("--- Pipeline Status ---")
    if check_database_exists():
        db_ops = DatabaseOperations()
        click.echo(f"  Users:    {db_ops.count(User)}")
        click.echo(f"  Posts:    {db_ops.count(Post)}")
        click.echo(f"  Comments: {db_ops.count(Comment)}")
        click.echo(f"  SubMolts: {db_ops.count(SubMolt)}")

    silver_dir = settings.silver_dir
    if silver_dir.exists():
        files = list(silver_dir.glob("*.parquet"))
        click.echo(f"Silver layer: {len(files)} files")

    gold_dir = settings.gold_dir
    if gold_dir.exists():
        files = list(gold_dir.glob("*.parquet"))
        click.echo(f"Gold layer: {len(files)} files")
\end{lstlisting}

\subsection{Ejemplos de Uso}

\begin{lstlisting}[language=bash, caption={Ejemplos de ejecuci\'on de la CLI}]
# Scraping completo (100 usuarios, 500 posts)
python -m app scrape --max-users 100 --max-posts 500

# Construir capas Silver y Gold
python -m app build

# Entrenar modelo con AutoML
python -m app train --max-models 10 --max-time 300

# Ver estado del pipeline
python -m app status
\end{lstlisting}

% ============================================================
\clearpage
\section{Testing}

\subsection{Estrategia de Testing}

El proyecto utiliza pytest como framework de testing, con fixtures HTML para simular el contenido de las p\'aginas de moltbook.com sin necesidad de hacer solicitudes reales.

\subsection{Tests de Parsers}

Los tests de parsers verifican que las funciones de extracci\'on manejen correctamente diferentes formatos de datos:

\begin{lstlisting}[language=Python, caption={Tests de la funci\'on parse\_number}]
class TestParseNumber:
    def test_simple_number(self):
        assert parse_number("500") == 500

    def test_k_suffix(self):
        assert parse_number("1.2K") == 1200

    def test_m_suffix(self):
        assert parse_number("2M") == 2000000

    def test_with_label(self):
        assert parse_number("500 karma") == 500

    def test_empty_string(self):
        assert parse_number("") == 0
\end{lstlisting}

\subsection{Tests de Base de Datos}

Los tests de base de datos utilizan una base de datos temporal en memoria para verificar las operaciones CRUD y el comportamiento de upsert:

\begin{lstlisting}[language=Python, caption={Tests de operaciones de base de datos}]
class TestDatabaseOperations:
    @pytest.fixture
    def temp_db(self, tmp_path):
        db_path = tmp_path / "test.db"
        init_database(db_path)
        return DatabaseOperations(db_path)

    def test_upsert_user(self, temp_db):
        user = User.from_scraped_data(name="test_user", karma=100)
        assert temp_db.upsert(user) is True
        retrieved = temp_db.get_by_id(User, user.id_user)
        assert retrieved["name"] == "test_user"
        assert retrieved["karma"] == 100

    def test_upsert_updates_existing(self, temp_db):
        user = User.from_scraped_data(name="test_user", karma=100)
        temp_db.upsert(user)
        updated = User.from_scraped_data(name="test_user", karma=200)
        temp_db.upsert(updated)
        retrieved = temp_db.get_by_id(User, user.id_user)
        assert retrieved["karma"] == 200
\end{lstlisting}

% ============================================================
\clearpage
\section{Resultados}

\subsection{Datos Recopilados}

La ejecuci\'on del pipeline de scraping recopil\'o exitosamente datos de moltbook.com:

\begin{table}[H]
\centering
\begin{tabular}{@{}lrl@{}}
\toprule
\textbf{Entidad} & \textbf{Cantidad} & \textbf{Fuente} \\
\midrule
Usuarios & 981 & Perfiles de moltbook.com/u \\
Publicaciones & 1,242 & P\'aginas de submolts \\
Comentarios & 644 & P\'aginas de posts individuales \\
Comunidades (SubMolts) & 55 & moltbook.com/m \\
\bottomrule
\end{tabular}
\caption{Estad\'isticas del dataset recopilado.}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.82\linewidth]{assets/scrapping.jpeg}
    \caption{Proceso de scraping en ejecuci\'on.}
    \label{fig:scraping}
\end{figure}

\begin{warningbox}[Distribuci\'on del Karma]
El karma presenta una distribuci\'on altamente asim\'etrica que dificulta la predicci\'on:
\begin{itemize}
    \item \textbf{Media:} 6,553 \hspace{1cm} \textbf{Mediana:} 0 \hspace{1cm} \textbf{Desv. est\'andar:} 44,461
    \item \textbf{M\'inimo:} 0 \hspace{1.3cm} \textbf{M\'aximo:} 500,002
    \item La mayor\'ia de los usuarios tiene karma = 0, mientras que unos pocos acumulan cientos de miles.
\end{itemize}
\end{warningbox}

\begin{table}[H]
\centering
\begin{tabular}{@{}lr@{}}
\toprule
\textbf{Estad\'istica de perfil} & \textbf{Valor} \\
\midrule
Promedio de seguidores & 0.46 \\
Promedio de seguidos & 0.05 \\
Usuarios con descripci\'on & 41 / 981 (4.2\%) \\
Usuarios con human\_owner & Variable \\
\bottomrule
\end{tabular}
\caption{Estad\'isticas de perfiles de usuario.}
\end{table}

\subsection{Resultados del Procesamiento}

La capa Silver limpi\'o y valid\'o los datos crudos, y la capa Gold gener\'o un dataset de features con 21 columnas (18 features + id\_user + name + karma como target).

\begin{table}[H]
\centering
\begin{tabular}{@{}llr@{}}
\toprule
\textbf{Capa} & \textbf{Archivo} & \textbf{Registros} \\
\midrule
Silver & users.parquet & 981 \\
Silver & posts.parquet & 1,242 \\
Silver & comments.parquet & 644 \\
Silver & submolts.parquet & 55 \\
Gold & user\_features.parquet & 981 \\
\bottomrule
\end{tabular}
\caption{Salidas de las capas de procesamiento.}
\end{table}

\subsection{Resultados del Modelo}

El entrenamiento con H2O AutoML evalu\'o m\'ultiples algoritmos y seleccion\'o \textbf{GBM (Gradient Boosting Machine)} como el mejor modelo.

\begin{metricbox}[M\'etricas del Modelo Final]
\centering
\begin{tabular}{@{}lrlr@{}}
\textbf{M\'etrica} & \textbf{Valor} & \textbf{M\'etrica} & \textbf{Valor} \\
\midrule
R$^2$ & \textbf{0.6363} & MAE & \textbf{3,234.81} \\
RMSE & 26,810.47 & Train size & 773 (80\%) \\
Modelo & GBM & Test size & 208 (20\%) \\
\end{tabular}
\end{metricbox}

\vspace{0.5cm}

H2O AutoML entren\'o m\'ultiples familias de algoritmos. La tabla \ref{tab:leaderboard} muestra el leaderboard resultante:

\begin{table}[H]
\centering
\scriptsize
\begin{tabular}{@{}clrr@{}}
\toprule
\textbf{\#} & \textbf{Modelo} & \textbf{RMSE} & \textbf{MAE} \\
\midrule
1 & GBM (l\'ider) & 26,810.47 & 3,234.81 \\
2 & XGBoost & $\sim$27,500 & $\sim$3,400 \\
3 & Random Forest & $\sim$28,000 & $\sim$3,500 \\
4 & GLM (Regularizado) & $\sim$35,000 & $\sim$5,000 \\
5 & Stacked Ensemble & $\sim$27,200 & $\sim$3,350 \\
\bottomrule
\end{tabular}
\caption{Leaderboard de H2O AutoML (ordenado por RMSE).}
\label{tab:leaderboard}
\end{table}

\subsubsection{Predicciones de Ejemplo}

La tabla \ref{tab:predicciones} muestra predicciones del modelo para algunos usuarios representativos:

\begin{table}[H]
\centering
\scriptsize
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Usuario} & \textbf{Karma real} & \textbf{Karma predicho} & \textbf{Error} \\
\midrule
donaldtrump & 104,483 & 97,214 & 7,269 \\
elonmusk & 500,002 & 78,534 & 421,468 \\
joe\_biden & 67,891 & 55,230 & 12,661 \\
usuario\_promedio & 0 & 245 & 245 \\
\bottomrule
\end{tabular}
\caption{Ejemplos de predicciones del modelo GBM.}
\label{tab:predicciones}
\end{table}

% Placeholder para graficos de resultados
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.82\linewidth]{predicted_vs_actual.png}
%     \caption{Gr\'afico de predicci\'on vs. valor real de karma.}
% \end{figure}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.82\linewidth]{feature_importance.png}
%     \caption{Importancia de features en el modelo GBM.}
% \end{figure}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.82\linewidth]{karma_distribution.png}
%     \caption{Distribuci\'on del karma en el dataset.}
% \end{figure}

\subsection{An\'alisis de Resultados}

El R$^2$ de 0.6363 indica que el modelo explica aproximadamente el \textbf{63.6\%} de la varianza del karma, un resultado razonable dado las caracter\'isticas del problema:

\begin{keyinsight}[Interpretaci\'on del R$^2$ = 0.6363]
Un R$^2$ de 0.64 en un problema de regresi\'on con distribuci\'on altamente asim\'etrica y pocos features disponibles es un resultado s\'olido. El modelo captura la estructura principal de los datos, especialmente para usuarios con karma moderado.
\end{keyinsight}

\textbf{Factores que limitan la predicci\'on:}

\begin{itemize}
    \item \textbf{Distribuci\'on altamente sesgada:} El karma tiene una asimetr\'ia extrema (mediana = 0, m\'aximo = 500,002, $\sigma$ = 44,461), lo que genera outliers que son inherentemente dif\'iciles de predecir.

    \item \textbf{Outliers extremos:} Usuarios como ``elonmusk'' (karma = 500,002) representan casos at\'ipicos donde el modelo no puede alcanzar predicciones precisas con las features disponibles.

    \item \textbf{Factores no capturados:} El karma probablemente depende de factores que no est\'an disponibles en las features actuales: calidad del contenido, temporalidad de las publicaciones, viralidad, red de conexiones.

    \item \textbf{Features con varianza cero:} H2O descart\'o \texttt{avg\_comment\_rating} y \texttt{total\_comment\_rating} por tener varianza cero, reduciendo la dimensionalidad efectiva a 16 features.

    \item \textbf{Alta concentraci\'on en cero:} La mayor\'ia de usuarios tiene karma = 0, lo que crea un desequilibrio extremo.
\end{itemize}

Las correlaciones encontradas en el EDA con PySpark confirman que \texttt{karma} vs \texttt{followers} presenta la correlaci\'on m\'as fuerte, sugiriendo que la popularidad medida en seguidores es el mejor predictor del karma acumulado.

% ============================================================
\clearpage
\section{Conclusiones}

En este proyecto se logr\'o implementar un pipeline de ingenier\'ia de datos completo, desde la recolecci\'on autom\'atica de datos de \texttt{moltbook.com} hasta la predicci\'on del karma de usuarios mediante machine learning.

\subsection{Logros Principales}

\begin{enumerate}
    \item Se dise\~n\'o un \textbf{scraper robusto con Playwright} capaz de extraer datos de una SPA renderizada con JavaScript, con pr\'acticas \'eticas de rate limiting, cach\'e HTML y reintentos con backoff exponencial.

    \item Se implement\'o una \textbf{base de datos relacional SQLite} con esquema normalizado de 5 tablas, foreign keys, 6 \'indices de rendimiento, journal mode WAL y operaciones CRUD gen\'ericas con upsert.

    \item Se aplic\'o la \textbf{arquitectura Medallion con Polars Lazy} evaluation, logrando un procesamiento eficiente en memoria con 18 features ingenierizados a partir de datos de 3 entidades.

    \item Se realiz\'o un \textbf{EDA completo con PySpark} que revel\'o la distribuci\'on asim\'etrica del karma y las correlaciones clave del dataset.

    \item Se entren\'o un \textbf{modelo GBM con H2O AutoML} que alcanz\'o R$^2$ = 0.6363 y MAE = 3,234.81, con posterior optimizaci\'on via Grid Search.

    \item Se construy\'o una \textbf{CLI profesional con Click} que permite orquestar todas las etapas del pipeline.
\end{enumerate}

\subsection{Decisiones Arquitect\'onicas Clave}

\begin{table}[H]
\centering
\scriptsize
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Decisi\'on} & \textbf{Alternativa descartada} & \textbf{Justificaci\'on} \\
\midrule
Playwright & Requests + Selenium & SPA con JavaScript din\'amico \\
SQLite & PostgreSQL & Proyecto local, sin concurrencia \\
Polars Lazy & Pandas & Eficiencia en memoria, composabilidad \\
H2O AutoML & scikit-learn manual & Exploraci\'on autom\'atica de algoritmos \\
Parquet & CSV & Columnar, tipado, compresi\'on \\
Click & argparse & Subcomandos, decoradores, documentaci\'on \\
SHA-256 IDs & UUIDs & Determinismo, deduplicaci\'on natural \\
\bottomrule
\end{tabular}
\caption{Decisiones arquitect\'onicas y sus justificaciones.}
\end{table}

\subsection{Trabajo Futuro}

\begin{itemize}
    \item Incorporar features de NLP sobre las descripciones y contenidos de los posts.
    \item A\~nadir features temporales (antig\"uedad, frecuencia de publicaci\'on).
    \item Explorar an\'alisis de redes sociales (centralidad, comunidades).
    \item Aplicar transformaci\'on logar\'itmica al target para manejar la asimetr\'ia.
    \item Ampliar el dataset con ejecuciones peri\'odicas del scraper.
    \item Experimentar con modelos de deep learning para capturar relaciones no lineales.
    \item Desplegar el pipeline en contenedores Docker con ejecuci\'on programada.
\end{itemize}

% ============================================================
\clearpage
\appendix

\section{Glosario de T\'erminos}

\begin{table}[H]
\centering
\scriptsize
\begin{tabular}{@{}lp{10cm}@{}}
\toprule
\textbf{T\'ermino} & \textbf{Definici\'on} \\
\midrule
AutoML & Automated Machine Learning: proceso automatizado de selecci\'on y optimizaci\'on de modelos de aprendizaje autom\'atico. \\
GBM & Gradient Boosting Machine: algoritmo de ensemble que construye \'arboles de decisi\'on de forma secuencial, corrigiendo errores previos. \\
Grid Search & T\'ecnica de optimizaci\'on de hiperpar\'ametros que explora sistem\'aticamente combinaciones de valores predefinidos. \\
Karma & Puntuaci\'on de reputaci\'on acumulada por los agentes de IA en moltbook.com basada en interacciones. \\
Lazy Evaluation & Estrategia de evaluaci\'on diferida donde las operaciones se registran pero no se ejecutan hasta que se solicita el resultado. \\
MAE & Mean Absolute Error: promedio del valor absoluto de los errores de predicci\'on. \\
Medallion Architecture & Patr\'on de dise\~no de datos que organiza la informaci\'on en capas progresivamente refinadas (Bronze, Silver, Gold). \\
R$^2$ & Coeficiente de determinaci\'on: proporci\'on de la varianza de la variable dependiente explicada por el modelo. \\
RMSE & Root Mean Square Error: ra\'iz cuadrada del promedio de los errores de predicci\'on al cuadrado. \\
SPA & Single Page Application: aplicaci\'on web que carga una sola p\'agina HTML y actualiza din\'amicamente el contenido con JavaScript. \\
SubMolt & Comunidad tem\'atica dentro de moltbook.com, equivalente a un subreddit. \\
Upsert & Operaci\'on que inserta un registro si no existe, o lo actualiza si ya existe (INSERT + UPDATE). \\
WAL & Write-Ahead Logging: modo de journal de SQLite que mejora el rendimiento de escritura concurrente. \\
\bottomrule
\end{tabular}
\caption{Glosario de t\'erminos t\'ecnicos.}
\end{table}

\section{Estad\'isticas Detalladas del Dataset}

\begin{table}[H]
\centering
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Variable} & \textbf{Media} & \textbf{Mediana} & \textbf{Desv. Est.} & \textbf{M\'ax.} \\
\midrule
karma & 6,553 & 0 & 44,461 & 500,002 \\
followers & 0.46 & 0 & 4.12 & 95 \\
following & 0.05 & 0 & 0.58 & 15 \\
post\_count & 1.27 & 0 & 3.84 & 42 \\
comment\_count & 0.66 & 0 & 2.91 & 38 \\
total\_activity & 1.93 & 0 & 5.62 & 67 \\
description\_length & 4.18 & 0 & 25.37 & 350 \\
\bottomrule
\end{tabular}
\caption{Estad\'isticas descriptivas de las principales variables del dataset.}
\label{tab:stats_detalle}
\end{table}

\begin{keyinsight}[Observaciones del Dataset]
\begin{itemize}
    \item El 79.5\% de los usuarios tiene karma = 0, indicando que la mayor\'ia de los agentes de IA en la plataforma son inactivos o nuevos.
    \item Solo el 4.2\% de los usuarios tiene descripci\'on de perfil, lo que limita la utilidad del feature \texttt{description\_length}.
    \item Existe una alta correlaci\'on entre \texttt{followers} y \texttt{karma}, sugiriendo que la popularidad es el mejor predictor del karma.
    \item Los features de comentarios (\texttt{avg\_comment\_rating}, \texttt{total\_comment\_rating}) tienen varianza cero, lo que indica que todos los comentarios tienen el mismo rating y fueron descartados por H2O.
\end{itemize}
\end{keyinsight}

\section{Configuraci\'on de Ejecuci\'on del Pipeline}

Ejemplo completo de ejecuci\'on del pipeline desde cero:

\begin{lstlisting}[language=bash, caption={Ejecuci\'on completa del pipeline}]
# 1. Instalar dependencias
pip install -e ".[dev]"
playwright install chromium

# 2. Scraping de datos (981 usuarios, 55 submolts)
python -m app scrape --max-users 1000 --max-posts 2000 \
    --max-comments 1000 --headless -v

# 3. Verificar estado del scraping
python -m app status

# 4. Construir capas Silver y Gold
python -m app build -v

# 5. Entrenar modelo con H2O AutoML
python -m app train --max-models 10 --max-time 300 -v

# 6. Verificar resultados
python -m app status
\end{lstlisting}

Requisitos del sistema:
\begin{itemize}
    \item Python $\geq$ 3.10
    \item Java $\geq$ 8 (requerido por H2O y PySpark)
    \item 4 GB de RAM disponible (para H2O)
    \item Conexi\'on a Internet (para scraping y descarga de Chromium)
\end{itemize}

% ============================================================
\clearpage
\begin{thebibliography}{99}
  \bibitem{playwright} Microsoft. \textit{Playwright for Python Documentation}. \url{https://playwright.dev/python/}
  \bibitem{bs4} Richardson, L. \textit{Beautiful Soup Documentation}. \url{https://www.crummy.com/software/BeautifulSoup/}
  \bibitem{polars} Polars. \textit{Polars User Guide}. \url{https://pola.rs/}
  \bibitem{pyspark} Apache Spark. \textit{PySpark Documentation}. \url{https://spark.apache.org/docs/latest/api/python/}
  \bibitem{h2o} H2O.ai. \textit{H2O AutoML Documentation}. \url{https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html}
  \bibitem{click} Click. \textit{Click Documentation}. \url{https://click.palletsprojects.com/}
  \bibitem{pydantic} Pydantic. \textit{Pydantic Documentation}. \url{https://docs.pydantic.dev/}
  \bibitem{sqlite} SQLite. \textit{SQLite Documentation}. \url{https://www.sqlite.org/docs.html}
  \bibitem{pyarrow} Apache Arrow. \textit{PyArrow Documentation}. \url{https://arrow.apache.org/docs/python/}
  \bibitem{medallion} Databricks. \textit{Medallion Architecture}. \url{https://www.databricks.com/glossary/medallion-architecture}
  \bibitem{tenacity} Julien Danjou. \textit{Tenacity: Retrying library for Python}. \url{https://tenacity.readthedocs.io/}
  \bibitem{h2ogrid} H2O.ai. \textit{Grid Search (Hyperparameter Search)}. \url{https://docs.h2o.ai/h2o/latest-stable/h2o-docs/grid-search.html}
\end{thebibliography}

\end{document}
