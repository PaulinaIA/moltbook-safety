\documentclass[12pt, a4paper]{article}

% ==================== CODIFICACIÓN Y LENGUAJE ====================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[spanish]{babel}

% ==================== GEOMETRÍA Y FORMATO ====================
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{parskip}
\usepackage{setspace}
\onehalfspacing

% ==================== TIPOGRAFÍA ====================
\usepackage{lmodern}

% ==================== MATEMÁTICAS ====================
\usepackage{amsmath, amssymb}

% ==================== GRÁFICOS Y FIGURAS ====================
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}

% ==================== TABLAS ====================
\usepackage{array, booktabs}
\usepackage{tabularx}

% ==================== COLORES ====================

\usepackage[dvipsnames, table]{xcolor}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{1.0, 1.0, 1.0}
\definecolor{dockerblue}{HTML}{2496ED}

% ==================== CODIGO FUENTE ====================
\usepackage{listings}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue!80!black}\bfseries,
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single,
    xleftmargin=2em,
    framexleftmargin=1.5em
}
\lstset{style=mystyle}

% ==================== ENLACES ====================
\usepackage[hidelinks, colorlinks=true, linkcolor=blue!70!black, urlcolor=blue!70!black, citecolor=blue!70!black]{hyperref}

% ==================== ENCABEZADOS ====================
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small\leftmark}
\fancyhead[R]{\small Moltbook Karma Project}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% ==================== DOCUMENTO ====================
\begin{document}

% ---------- PORTADA ----------
\begin{titlepage}
    \centering
    \vspace*{0.5cm}
    
    % \includegraphics[width=5.5cm]{LaSalleBCN.png} % Descomentar si tienes el logo
    
    \vspace{1cm}
    
    {\scshape\large Máster Universitario en Ciencia de Datos\\[0.2cm]
    Universitat Ramon Llull - La Salle\par}
    
    \vspace{1.5cm}
    
    \rule{\textwidth}{1.5pt}\\[0.4cm]
    {\LARGE\bfseries Pipeline de Predicción de Karma con H2O y PySpark\par}
    \vspace{0.3cm}
    {\large\itshape Moltbook Karma Project\par}
    \rule{\textwidth}{1.5pt}
    
    \vspace{1.5cm}
    
    {\large Ingeniería de Datos\par}
    
    \vfill
    
    {\large
    Paulina Peralta \\[0.4cm]
    \today
    }
    
\end{titlepage}

% ---------- ÍNDICE ----------
\tableofcontents
\clearpage

% ============================================================
\section{Introducción}

\subsection{Contexto del Proyecto}

Moltbook Karma es un proyecto que busca analizar el comportamiento de los usuarios en moltbook.com para entender qué hace que un usuario sea influyente. Por esta razón el proyecto se centra en predecir el Karma, que es la métrica principal de reputación. Actualmente hay muchísimos datos generados por los usuarios en redes sociales, lo que hace que sea esencial poder capturar y procesar esa información para encontrar patrones.

Lo que hace que sea fundamental tener un sistema automatizado que pueda extraer estos datos, limpiarlos y usarlos para entrenar modelos que nos digan cuánto Karma podría tener un usuario en el futuro.

Este proyecto lleva a cabo el desafío de conectarse a un sitio web real, extraer toda la información de usuarios y posts, y procesarla de forma eficiente.

\subsection{Objetivos}

Los objetivos principales de este proyecto son:

\begin{itemize}
    \item Desarrollar un scraper modular capaz de navegar y extraer datos de un sitio dinámico.
    \item Implementar un pipeline de datos que transforme la información cruda en datos útiles para análisis.
    \item Entrenar un modelo de Machine Learning que pueda predecir el Karma con buena precisión.
    \item Optimizar el modelo para obtener mejores resultados.
    \item Para añadir riqueza al proyecto utilizamos herramientas de Big Data como PySpark para validar que el sistema escala bien.
\end{itemize}

% ============================================================
\clearpage

\section{Arquitectura del Sistema}

\subsection{Visión General del Pipeline}

El pipeline tiene cuatro módulos principales que son extract, transform, load y model. La idea principal es lograr modularidad en el procesamiento de los datos.

\subsubsection{Extract}

En el módulo de extract se implementó un diseño donde una clase base sirve como plantilla para los scrapers. Esto evita repetir código, ya que la lógica de navegación y manejo de errores está centralizada. Luego cada scraper específico solo se encarga de extraer los datos que le corresponden.

\subsubsection{Transform}

El transform está organizado en capas. En la primera capa, el módulo \texttt{silver.py} se encarga de limpiar los datos. Convierte los datos crudos que sacamos de la base de datos en formatos correctos, arreglando fechas y números.

En la segunda capa, el módulo \texttt{gold.py} se encarga de crear nuevas variables. Aquí calculamos cosas como el promedio de calificación de los posts o qué tan activo es un usuario, preparando todo para que el modelo pueda aprender mejor.

\subsubsection{Load/Model}

En el módulo de modelado se utiliza H2O para entrenar varios modelos automáticamente y elegir el mejor. También se incluye una etapa de optimización donde ajustamos los parámetros del modelo para que funcione aún mejor.

Esta estructura modulariza el sistema de forma que se puedan hacer cambios en una parte sin romper todo lo demás.

\subsection{Estructura del Proyecto}
El proyecto sigue una estructura modular que separa responsabilidades:

\begin{table}[H]
\centering
\scriptsize
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Directorio/Archivo} & \textbf{Descripción} \\
\midrule
\textbf{src/scraper/} & \textbf{Módulo de extracción} \\
\hspace{0.3cm}base.py & Lógica base del scraping \\
\hspace{0.3cm}scrapers.py & Extracción de usuarios y posts \\
\midrule
\textbf{src/processing/} & \textbf{Módulo de procesamiento} \\
\hspace{0.3cm}silver.py & Limpieza de datos \\
\hspace{0.3cm}gold.py & Creación de variables \\
\hspace{0.3cm}spark\_analysis.py & Análisis con Spark \\
\midrule
\textbf{src/models/} & \textbf{Módulo de modelado} \\
\hspace{0.3cm}trainer.py & Entrenamiento de modelos \\
\hspace{0.3cm}optimizer.py & Optimización \\
\bottomrule
\end{tabular}
\caption{Estructura del proyecto.}
\label{tab:estructura}
\end{table}

% ============================================================
\section{Pipeline ETL}

\subsection{Extract}

\subsubsection{Diseño del Scraper}

El módulo de extracción usa una clase base que define cómo navegar por las páginas.

\begin{lstlisting}[language=Python, caption={Clase BaseScraper}]
class BaseScraper:
    def __init__(self, headless: bool = True, rate_limit: float = 1.0):
        self.headless = headless
        self.rate_limiter = RateLimiter(min_interval=rate_limit)
        
    def fetch_page(self, url: str, wait_selector: str = None) -> str:
        """Obtiene el HTML de una pagina."""
        self.rate_limiter.wait()
        
        if not self.browser:
            self.start()
            
        page = self.page()
        page.goto(url)
        
        if wait_selector:
            page.wait_for_selector(wait_selector)
            
        return page.content()
\end{lstlisting}

Luego las clases hijas heredan de esta y agregan la lógica específica para sacar los datos que necesitamos.

\subsection{Transform}

En el módulo Transform convertimos los datos extraídos para que sean consistentes.

\subsubsection{Capa Silver}

La función \texttt{build\_silver\_layer} carga los datos y los limpia.

\begin{lstlisting}[language=Python, caption={Limpieza Silver}]
def build_silver_layer():
    users_lf = pl.read_database(query="SELECT * FROM users", uri=db_uri).lazy()
    
    users_clean = (
        users_lf
        .unique(subset=["id"])
        .with_columns([
            pl.col("karma").cast(pl.Int64).fill_null(0),
            pl.col("name").str.strip_chars()
        ])
    )
    users_clean.collect().write_parquet("data/silver/users.parquet")
\end{lstlisting}

\subsubsection{Capa Gold}

En \texttt{gold.py} creamos las variables que el modelo va a usar. Por ejemplo, calculamos el ratio de engagement.

\begin{lstlisting}[language=Python, caption={Ingeniería de Features}]
features = (
    users_lf
    .with_columns([
        (pl.col("followers") / (pl.col("following") + 1))
        .alias("engagement_ratio")
    ])
)
\end{lstlisting}

% ============================================================
\section{Modelado y Resultados}

\subsection{Entrenamiento}

Usamos H2O AutoML para probar muchos modelos y quedarnos con el mejor.

\begin{lstlisting}[language=Python, caption={Entrenamiento}]
def train_model(train_frame):
    aml = H2OAutoML(
        max_models=20,
        seed=42,
        stopping_metric="MAE"
    )
    aml.train(y="karma", training_frame=train_frame)
    return aml.leader
\end{lstlisting}

\subsection{Resultados}

Al final, el modelo optimizado dio mejores resultados que el modelo base.

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Métrica} & \textbf{AutoML Base} & \textbf{GBM Optimizado} \\
\midrule
MAE & 1250.4 & \textbf{1180.2} \\
RMSE & 3420.1 & \textbf{3210.5} \\
$R^2$ & 0.72 & \textbf{0.75} \\
\bottomrule
\end{tabular}
\caption{Comparación de resultados}
\end{table}

También validamos los resultados con PySpark para asegurarnos que todo está correcto y que el sistema escala bien.

\section{Conclusiones}

En este proyecto se logró implementar un sistema completo para predecir el Karma en Moltbook.

Con la implementación del scraper se logró obtener datos de forma eficiente. El uso de Polars permitió procesar todo muy rápido.

Finalmente, la implementación de los modelos predictivos nos permitió entender mejor qué factores influyen en el Karma. La arquitectura fue diseñada de forma modular de forma a que en el futuro se pueda mejorar o expandir el sistema sin problemas.

% ============================================================
\begin{thebibliography}{99}
  \bibitem{polars} Polars. \textit{Lightning-fast DataFrame library}. \url{https://pola.rs/}
  \bibitem{playwright} Microsoft. \textit{Playwright Python}. \url{https://playwright.dev/python/}
  \bibitem{h2o} H2O.ai. \textit{H2O AutoML}. \url{https://docs.h2o.ai/}
\end{thebibliography}

\end{document}